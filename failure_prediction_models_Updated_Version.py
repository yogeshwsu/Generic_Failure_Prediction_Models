# -*- coding: utf-8 -*-
"""Failure_Prediction_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KLTh77eKVaei1-yvBY7LqFHx4bMB1WRZ

# **PROBLEM DEFINITION**

## Problem 1 - Identification of Event Type (Classification Problem)<br>
## Problem 2 - Prediction of Time between Failures (Regression Problem)<br>
## Problem 3 - Identification of Failing Node (Classification Problem)<br>
## Problem 4 - Prediction of Time to Return/Repair (Regression Problem)<br>

#**Dataset Used: Grid'5000 Dataset**

The dataset being analyzed originates from the Grid'5000 project, which is a large-scale and versatile experimental platform designed to support research in computer science, particularly in the areas of distributed and parallel computing. The dataset encapsulates a broad spectrum of information related to the nodes, components, platforms, events, and performance metrics of the Grid'5000 infrastructure, providing a detailed snapshot of its operational environment. All information captured are from 05/2005 to 11/2006

##**Key Components of the Dataset:**

**Component Dataset:**

Contains information about the individual components within the Grid'5000 infrastructure, including their identification, association with specific nodes and platforms, the entities responsible for their creation, and the temporal scope of their operational traces.

**Creator Dataset:**

Documents the creators of various components, offering insights into the attribution of work and potentially the citation or copyright information relevant to the datasets or experiments.

**Event State Dataset:**

Tracks the state of events associated with components, including integer, floating-point, and string values that describe specific event parameters.

**Event Trace Dataset:**

Provides a detailed record of events occurring within the Grid'5000 framework, including start and stop times, types of events, and reasons for their termination.

**Node Dataset:**

Describes the nodes in the Grid'5000 infrastructure, including details such as their geographic location, processing capabilities, memory, storage, bandwidth, and other technical specifications.

**Node Performance Dataset:**

Records performance metrics for the nodes, including various speed metrics and values that reflect the performance characteristics of the nodes during specific periods.

**Platform Dataset:**

Captures details about the platforms within Grid'5000, including their location, type, and any additional notes that may be relevant to understanding the context in which they operate.

**Research Implications:**
This dataset serves as a valuable resource for researchers investigating distributed systems, high-performance computing, and experimental computer science. By analyzing the dataset, researchers can gain insights into the performance and behavior of large-scale distributed infrastructures, assess the impact of different configurations and events on system performance, and develop new algorithms or models for improving the efficiency and reliability of such systems.
"""

!pwd

# Installing required modules
!pip install pandas matplotlib seaborn scikit-learn tensorflow statsmodels

"""#Loading and Pre Processing Grid'5000 Dataset"""

import pandas as pd

# Read CSV files with specific columns
def read_file(filename, sep='\t', skiprows=1, col_names=None):
    try:
        # Read file with the specified separator, skiprows, and column names
        df = pd.read_csv(filename, sep=sep, skiprows=skiprows, names=col_names)
        return df
    except pd.errors.EmptyDataError:
        print(f"No columns to parse from file: {filename}")
        return pd.DataFrame(columns=col_names)

# Define column names for each file
component_columns = ['component_id', 'node_id', 'platform_id', 'creator_id', 'node_name', 'component_type', 'trace_start', 'trace_end', 'resolution']
creator_columns = ['creator_id', 'component_id', 'node_id', 'platform_id', 'creator', 'cite', 'copyright']
event_state_columns = ['event_id', 'component_id', 'node_id', 'platform_id', 'i_val', 'f_val', 's_val']
event_trace_columns = ['event_id', 'component_id', 'node_id', 'platform_id', 'node_name', 'event_type', 'event_start_time', 'event_stop_time', 'event_end_reason']
node_columns = ['node_id', 'platform_id', 'node_name', 'node_ip', 'node_location', 'timezone', 'proc_model', 'os_name', 'cores_per_proc', 'num_procs', 'mem_size', 'disk_size', 'up_bw', 'down_bw', 'metric_id', 'notes']
node_perf_columns = ['metric_id', 'node_id', 'platform_id', 'sfpop_speed', 'dfpop_speed', 'iop_speed', 'i_val', 'f_val', 's_val']
platform_columns = ['platform_id', 'platform_name', 'platform_location', 'platform_type', 'notes']

# Read files with the specified column names
component = read_file('component.tab', col_names=component_columns)
creator = read_file('creator.tab', col_names=creator_columns)
event_state = read_file('event_state.tab', col_names=event_state_columns)
event_trace = read_file('event_trace.tab', col_names=event_trace_columns)
node = read_file('node.tab', col_names=node_columns)
node_perf = read_file('node_perf.tab', col_names=node_perf_columns)
platform = read_file('platform.tab', col_names=platform_columns)

# Display the first few rows of each dataset
print("Component Data:")
print(component.head())

print("\nCreator Data:")
print(creator.head())

print("\nEvent State Data:")
print(event_state.head())

print("\nEvent Trace Data:")
print(event_trace.head())

print("\nNode Data:")
print(node.head())

print("\nNode Performance Data:")
print(node_perf.head())

print("\nPlatform Data:")
print(platform.head())

# Function to print unique values for each column in a DataFrame
def print_unique_values(df, df_name):
    print(f"\nUnique values in {df_name}:")
    for column in df.columns:
        print(f"{column}: {df[column].unique()[:10]}")

# Print unique values for each dataset
print_unique_values(component, "Component")
print_unique_values(creator, "Creator")
print_unique_values(event_state, "Event State")
print_unique_values(event_trace, "Event Trace")
print_unique_values(node, "Node")
print_unique_values(node_perf, "Node Performance")
print_unique_values(platform, "Platform")

"""#**Overview of Structure**

The FTA data is structured hierarchically from the platform level down to individual event traces, capturing the state and performance of various components within nodes. The hierarchy is:

**Platform** → **Node** → **Component** → **Event Trace**

This hierarchical organization ensures that data is logically grouped, making it easier to analyze system behavior and performance at different levels.


![FTA structure.png](data:image/png;base64,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)

---


##**Detailed Breakdown**

**Platform**

A platform represents the entire computing environment or system being monitored. Examples include distributed systems like SETI@home or a corporate network of desktops. Each platform has a unique identifier (platform_id), a name, location, and type (e.g., cluster, grid, etc.).
Attributes include:
platform_id: Unique identifier for the platform.
platform_name, platform_location, platform_type: Descriptive fields to understand the platform's nature.
notes: Any additional information.


**Node**

A node is a physical or virtual machine within a platform, like a server or a workstation. Nodes host components and are uniquely identified within the platform by a node_id.
Attributes include:
node_id: Unique identifier for the node.
platform_id: The platform to which this node belongs.
node_name, node_ip, node_location: Identifying and locating information.
timezone, proc_model, os_name, cores_per_proc, num_procs, mem_size, disk_size, up_bw, down_bw: Hardware and network specifications.
metric_id, notes: Performance metrics and additional information.


**Component**

A component is a software or hardware resource within a node, like a CPU, memory module, or a network interface. Each component has a component_id and is associated with a node_id.
Attributes include:
component_id: Unique identifier for the component.
node_id, platform_id: Identifiers linking the component to its node and platform.
creator_id: Identifier of the person or entity responsible for the component's trace data.
component_type: Type of component (e.g., host, CPU, network).
trace_start, trace_end: Times when the component started and ended its trace recording.
resolution: The granularity of the data recorded by this component.


**Event Trace**

An event trace records the activity or state changes of a component over time. Each event within the trace is logged with a start and stop time and a type indicating what kind of event occurred (e.g., availability or unavailability).
Attributes include:
event_id: Unique identifier for the event.
component_id, node_id, platform_id: Identifiers linking the event to its component, node, and platform.
event_type: Type of event (e.g., availability, unavailability).
event_start_time, event_end_time: The time span during which the event occurred.
event_end_reason: Reason why the event ended (e.g., hardware failure, software error).


**Event State**

An event state provides details about the condition or performance of a component during an event trace. This could include metrics like CPU availability or network throughput.
Attributes include:
event_id: The event to which this state belongs.
component_id, node_id, platform_id: Identifiers linking the state to its component, node, and platform.
i_val, f_val, s_val: The recorded values for integer, float, and string metrics during the event.


In the Grid 5000 network, there are 9 geographical sites in France and Luxembourg, 15 clusters, and 1288 nodes.

9 sites labelled site1 through site9.

15 clusters distributed across the 9 sites, labelled as:
1. site1/c1
2. site1/c2
3. site1/c3
4. site1/c4
5. site2/c1
6. site3/c1
7. site4/c1
8. site4/c2
9. site5/c1
10. site5/c2
11. site6/c1
12. site6/c2
13. site7/c1
14. site8/c1
15. site9/c1

And 1288 uniquely ID'd nodes.
"""

# Display summary statistics for each dataframe
print("Component Summary Statistics:")
print(component.describe())

print("\nCreator Summary Statistics:")
print(creator.describe())

print("\nEvent Trace Summary Statistics:")
print(event_trace.describe())

print("\nNode Summary Statistics:")
print(node.describe())

print("\nPlatform Summary Statistics:")
print(platform.describe())

"""Remove Empty Datasets"""

import os
import pandas as pd

# Function to check if a file is empty and remove it
def remove_empty_file(filename):
    try:
        df = pd.read_csv(filename, sep='\t', skiprows=1)
        if df.empty:
            os.remove(filename)
            print(f"Removed empty dataset: {filename}")
        else:
            print(f"Dataset {filename} is not empty.")
    except pd.errors.EmptyDataError:
        os.remove(filename)
        print(f"Removed empty dataset (EmptyDataError): {filename}")

# Check and remove empty datasets
for filename in ['event_state.tab', 'node_perf.tab']:
    if os.path.exists(filename):
        remove_empty_file(filename)
    else:
        print(f"File {filename} does not exist.")

# Check for missing values in each dataframe
print("Component Missing Values:")
print(component.isnull().sum())

print("\nCreator Missing Values:")
print(creator.isnull().sum())

print("\nEvent Trace Missing Values:")
print(event_trace.isnull().sum())

print("\nNode Missing Values:")
print(node.isnull().sum())

print("\nPlatform Missing Values:")
print(platform.isnull().sum())

"""No missing values found in any of the datasets"""

#Remove Columns with Only NULL Values in node.tab
columns_to_remove = ['node_ip', 'proc_model', 'os_name', 'cores_per_proc', 'mem_size', 'disk_size', 'up_bw', 'down_bw', 'metric_id', 'notes']
node = node.drop(columns=columns_to_remove)

print("Node Data after cleaning:")
print(node.head())

import pandas as pd

# Function to categorize columns
def categorize_columns(df):
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns.tolist()
    return numeric_cols, categorical_cols

# Component dataset
component_numeric_cols, component_categorical_cols = categorize_columns(component)
print("Component Numeric Columns:", component_numeric_cols)
print("Component Categorical Columns:", component_categorical_cols)

# Creator dataset
creator_numeric_cols, creator_categorical_cols = categorize_columns(creator)
print("\nCreator Numeric Columns:", creator_numeric_cols)
print("Creator Categorical Columns:", creator_categorical_cols)

# Event Trace dataset
event_trace_numeric_cols, event_trace_categorical_cols = categorize_columns(event_trace)
print("\nEvent Trace Numeric Columns:", event_trace_numeric_cols)
print("Event Trace Categorical Columns:", event_trace_categorical_cols)

# Node dataset
node_numeric_cols, node_categorical_cols = categorize_columns(node)
print("\nNode Numeric Columns:", node_numeric_cols)
print("Node Categorical Columns:", node_categorical_cols)

# Platform dataset
platform_numeric_cols, platform_categorical_cols = categorize_columns(platform)
print("\nPlatform Numeric Columns:", platform_numeric_cols)
print("Platform Categorical Columns:", platform_categorical_cols)

# Load component and event_trace datasets
component_columns = ['component_id', 'node_id', 'platform_id', 'creator_id', 'node_name', 'component_type', 'trace_start', 'trace_end', 'resolution']
event_trace_columns = ['event_id', 'component_id', 'node_id', 'platform_id', 'node_name', 'event_type', 'event_start_time', 'event_stop_time', 'event_end_reason']

component = pd.read_csv('component.tab', sep='\t', skiprows=1, names=component_columns)
event_trace = pd.read_csv('event_trace.tab', sep='\t', skiprows=1, names=event_trace_columns)

# Strip leading and trailing spaces from node_name
component['node_name'] = component['node_name'].str.strip()
event_trace['node_name'] = event_trace['node_name'].str.strip()

# Save the cleaned datasets
component.to_csv('cleaned_component.tab', sep='\t', index=False)
event_trace.to_csv('cleaned_event_trace.tab', sep='\t', index=False)

print("Component Data after cleaning:")
print(component.head())
print("\nEvent Trace Data after cleaning:")
print(event_trace.head())

# Convert time columns in component dataset
component['trace_start'] = pd.to_datetime(component['trace_start'], unit='s')
component['trace_end'] = pd.to_datetime(component['trace_end'], unit='s')

# Convert time columns in event_trace dataset
event_trace['event_start_time'] = pd.to_datetime(event_trace['event_start_time'], unit='s')
event_trace['event_stop_time'] = pd.to_datetime(event_trace['event_stop_time'], unit='s')

# Save the updated datasets
component.to_csv('cleaned_component.tab', sep='\t', index=False)
event_trace.to_csv('cleaned_event_trace.tab', sep='\t', index=False)

print("Component Data with datetime format:")
print(component.head())
print("\nEvent Trace Data with datetime format:")
print(event_trace.head())

"""## Removal of duplicate records"""

import pandas as pd

# Count records before removing duplicates
component_count_before = component.shape[0]
creator_count_before = creator.shape[0]
event_trace_count_before = event_trace.shape[0]
node_count_before = node.shape[0]
platform_count_before = platform.shape[0]

# Remove duplicates in all datasets
component.drop_duplicates(inplace=True)
creator.drop_duplicates(inplace=True)
event_trace.drop_duplicates(inplace=True)
node.drop_duplicates(inplace=True)
platform.drop_duplicates(inplace=True)

# Count records after removing duplicates
component_count_after = component.shape[0]
creator_count_after = creator.shape[0]
event_trace_count_after = event_trace.shape[0]
node_count_after = node.shape[0]
platform_count_after = platform.shape[0]

# Display the counts before and after
print("Record counts before and after removing duplicates:\n")
print(f"Component - Before: {component_count_before}, After: {component_count_after}")
print(f"Creator   - Before: {creator_count_before}, After: {creator_count_after}")
print(f"Event Trace - Before: {event_trace_count_before}, After: {event_trace_count_after}")
print(f"Node      - Before: {node_count_before}, After: {node_count_after}")
print(f"Platform  - Before: {platform_count_before}, After: {platform_count_after}")

print("\nComponent Data after removing duplicates:")
print(component.head())

"""## Merge the datasets to form a single merged dataframe"""

import pandas as pd

# Merge component and event_trace datasets on common columns
merged_data = pd.merge(component, event_trace, on=['component_id', 'node_id', 'platform_id', 'node_name'], how='inner')

# Merge with node dataset
merged_data = pd.merge(merged_data, node, on=['node_id', 'platform_id'], how='inner')

# Merge with creator dataset
merged_data_with_creator = pd.merge(merged_data, creator, on=['component_id', 'node_id', 'platform_id'], how='left')

# Merge with platform dataset
final_merged_data = pd.merge(merged_data_with_creator, platform, on='platform_id', how='left')

# Display the final merged data
print("Final Merged Data:")
print(final_merged_data.head())

# Save the final merged dataset
final_merged_data.to_csv('final_merged_data.tab', sep='\t', index=False)

columns_to_drop = ['creator', 'cite', 'copyright', 'platform_name', 'platform_location', 'platform_type','notes', 'creator_id_x','creator_id_y']

final_merged_data.drop(columns=columns_to_drop, inplace=True)

final_merged_data.columns

"""**17 Features**"""

final_merged_data.info()

final_merged_data.describe()

# Separate numeric and categorical columns
numeric_columns = final_merged_data.select_dtypes(include=['number']).columns
categorical_columns = final_merged_data.select_dtypes(include=['object', 'category']).columns
datetime_columns = final_merged_data.select_dtypes(include=['datetime64[ns]']).columns

print("Numeric Columns:")
print(numeric_columns)
print("Categoric Columns: ")
print(categorical_columns)
print("DateTime Columns: ")
print(datetime_columns)

print(final_merged_data.info())

# Loop through each column and print unique values, their count, and frequency
for column in final_merged_data.columns:
    print(f"Column: {column}")
    print(f"Number of Unique Values: {final_merged_data[column].nunique()}")
    print("Value Counts:")
    print(final_merged_data[column].value_counts().head(10))  # Display top 10 most frequent values
    print("-" * 50)

"""## Number of features with only 1 unique value:

1. component_id
2. component_type
3. platform_id
4. trace_start
5. trace_end
6. event_end_reason
7. time_zone
8. num_procs
"""

# List of columns with only one unique value
columns_to_drop = ['component_id', 'component_type', 'platform_id',
                   'trace_start', 'trace_end', 'event_end_reason',
                   'timezone', 'num_procs', 'node_name_y', 'resolution']

# Drop these columns from the dataframe
final_merged_data = final_merged_data.drop(columns=columns_to_drop, axis=1)

# Rename the column 'node_name_x' to 'node_name'
final_merged_data = final_merged_data.rename(columns={'node_name_x': 'node_name'})

# Confirm the changes
print("Updated dataframe columns:")
print(final_merged_data.columns)

"""## Updated features:

1. 'node_id'
2. 'node_name'
3. 'event_id'
4. 'event_type'
5. 'event_start_time'
6. 'event_stop_time'
7. 'node_location'

##**Numeric Columns**:
These columns consist of numerical data types, either integers or floats, representing different system and event attributes.

1. node_id: Represents the unique identifier of the node in the distributed system.

2. event_id: A unique identifier for each event. It helps in tracking and distinguishing between various events in the system.

3. event_type: A numerical representation of the type of event (e.g., failure, restart). Could indicate different failure modes.

##**Categorical Columns:**
These are columns that contain discrete categories or labels, often represented as text strings or specific labels. They indicate non-numeric attributes that describe the system.

1. node_name: The name of the node involved in the event.

2. node_location: A label representing the physical or logical location of the node in the distributed system.

##**DateTime Columns**:
These columns contain timestamps or time-based data, capturing when events occurred and ended.

1. event_start_time: The specific start time of the event. It’s useful for determining when a fault or failure occurred.

2. event_stop_time: The specific stop time of the event, helping determine the duration of the event or failure.
"""

final_merged_data.head(5)

final_merged_data.info()

"""#**Exploratory Data Analysis**"""

print(final_merged_data.describe())

import pandas as pd

# Identify categorical columns
categorical_columns = final_merged_data.select_dtypes(include=['object', 'category']).columns

# Print unique value counts for each categorical column
for column in categorical_columns:
    print(f"Unique values in {column}:")
    print(final_merged_data[column].value_counts())
    print("\n")

import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

# Separate numeric and categorical columns
numeric_columns = final_merged_data.select_dtypes(include=['number']).columns
categorical_columns = final_merged_data.select_dtypes(include=['object', 'category']).columns

# Plot histograms for numeric columns one by one
print("Histograms for Numeric Columns:")
for column in numeric_columns:
    plt.figure(figsize=(10, 6))
    final_merged_data[column].hist(bins=30, edgecolor='black')
    plt.title(f'Distribution of {column}', fontsize=30)
    plt.xlabel(column, fontsize=30)
    plt.ylabel('Frequency', fontsize=30)

    ax = plt.gca()

    y_formatter = ScalarFormatter(useMathText=True)
    y_formatter.set_powerlimits((0, 0))
    ax.yaxis.set_major_formatter(y_formatter)

    x_formatter = ScalarFormatter(useMathText=True)
    x_formatter.set_powerlimits((0, 0))
    ax.xaxis.set_major_formatter(x_formatter)


    #formatter = ScalarFormatter(useMathText=True)
    #formatter.set_powerlimits((0, 0))  # Force scientific notation always
    #ax.yaxis.set_major_formatter(formatter)
    #ax.xaxis.set_major_formatter(formatter)

    plt.xticks(fontsize=30)
    plt.yticks(fontsize=30)

    # Increase font size of the scientific notation exponent
    ax.yaxis.get_offset_text().set_fontsize(30)
    ax.xaxis.get_offset_text().set_fontsize(30)

    plt.tight_layout()
    plt.savefig(f'{column}_distribution.png', dpi=300, bbox_inches='tight', pad_inches=.05)
    plt.show()

# Plot bar charts for categorical columns one by one
print("Bar Charts for Categorical Columns:")
for column in categorical_columns:
    plt.figure(figsize=(10, 6))
    final_merged_data[column].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

fig, ax = plt.subplots(figsize=(10, 6))

value_counts = final_merged_data['node_location'].value_counts()
ax.bar(value_counts.index, value_counts.values, edgecolor='black')

# Remove 'G1/' prefix
labels_no_g1 = [label.split('/', 1)[1] if '/' in label else label for label in value_counts.index]

# Replace 'site' with 'S'
labels_cleaned = [lbl.replace('site', 'S') for lbl in labels_no_g1]

ax.set_title('Cluster wise Events Count', fontsize=30)
#plt.title('Frequency of Recorded Events by Clusters', fontsize=30)
ax.set_xlabel('Cluster Name', fontsize=30)
ax.set_ylabel('Events Count', fontsize=30)

y_formatter = ScalarFormatter(useMathText=True)
y_formatter.set_powerlimits((0, 0))
ax.yaxis.set_major_formatter(y_formatter)

ax.set_xticks(range(len(labels_cleaned)))
ax.set_xticklabels(labels_cleaned, rotation=75, ha='right', fontsize=30)
#ax.tick_params(axis='x', pad=20)

ax.yaxis.get_offset_text().set_fontsize(30)

plt.setp(ax.get_yticklabels(), fontsize=30)

ax.grid(axis='y', linestyle='--', alpha=0.7)


plt.tight_layout()

plt.savefig('node_location_distribution.png', dpi=300, bbox_inches='tight', pad_inches=0)

plt.show()

'''
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

# Bar chart for the 'node_location' categorical feature
#plt.figure(figsize=(12, 8))
plt.figure(figsize=(10, 6))
final_merged_data['node_location'].value_counts().plot(
    kind='bar', edgecolor='black'
)
plt.title('Frequency of Recorded Events by Clusters', fontsize=30)
plt.xlabel('Cluster', fontsize=30)  # Name for the x-axis
plt.ylabel('Events Count', fontsize=30)  # Name for the y-axis

y_formatter = ScalarFormatter(useMathText=True)
y_formatter.set_powerlimits((0, 0))
ax.yaxis.set_major_formatter(y_formatter)

x_formatter = ScalarFormatter(useMathText=True)
x_formatter.set_powerlimits((0, 0))
ax.xaxis.set_major_formatter(x_formatter)

plt.xticks(rotation=45, ha='right', fontsize=30)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
'''

import matplotlib.pyplot as plt

# Group by 'node_location' and count the number of unique 'node_name' values in each cluster
node_counts_per_cluster = final_merged_data.groupby('node_location')['node_name'].nunique()

# Sort the counts descending as before for plotting
node_counts_sorted = node_counts_per_cluster.sort_values(ascending=False)

# Create figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Plot bars manually with sorted counts
ax.bar(range(len(node_counts_sorted)), node_counts_sorted.values,
       edgecolor='black')

# Original labels (full node_location strings)
labels = node_counts_sorted.index.tolist()

# Process labels: remove "G1/", then replace "site" with "S"
def clean_label(label):
    if '/' in label:
        label = label.split('/', 1)[1]  # remove 'G1/'
    return label.replace('site', 'S')

labels_cleaned = [clean_label(lbl) for lbl in labels]

# Set x-axis ticks and corresponding cleaned labels
ax.set_xticks(range(len(labels_cleaned)))
ax.set_xticklabels(labels_cleaned, rotation=75, ha='right', fontsize=30)

# Set titles and axis labels
ax.set_title('Nodes Count per Cluster', fontsize=30)
ax.set_xlabel('Cluster Name', fontsize=30)
ax.set_ylabel('Nodes Count', fontsize=30)

# Show grid for y-axis
ax.grid(axis='y', linestyle='--', alpha=0.7)

# Adjust layout
plt.tight_layout()

plt.savefig('node_count_distribution.png', dpi=300, bbox_inches='tight', pad_inches=0)

plt.show()

import seaborn as sns

# Compute the correlation matrix-
correlation_matrix = final_merged_data[numeric_columns].corr()

# Plot heatmap of the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# Plot scatter plot matrix
sns.pairplot(final_merged_data[numeric_columns])
plt.show()

"""##**Time Series Analysis**"""

final_merged_data['event_start_time'] = pd.to_datetime(final_merged_data['event_start_time'], unit='s')
final_merged_data.set_index('event_start_time').resample('D').size().plot()
plt.show()

"""## Box plots to detect outliers"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot box plots for all numeric columns to detect outliers
for col in numeric_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=final_merged_data, y=col)
    plt.title(f'Box Plot of {col}')
    plt.grid(True)
    plt.show()

"""#**Statistical Analysis**"""

import pandas as pd
from scipy.stats import chi2_contingency

# Perform Chi-Square Tests for all pairs of categorical columns
for cat_col1 in categorical_columns:
    for cat_col2 in categorical_columns:
        if cat_col1 != cat_col2:
            contingency_table = pd.crosstab(final_merged_data[cat_col1], final_merged_data[cat_col2])
            chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

            print(f"\nChi-Square Test between {cat_col1} and {cat_col2}:")
            print(f"Chi-Square Statistic: {chi2_stat}")
            print(f"P-Value: {p_val}")
            print(f"Degrees of Freedom: {dof}")
            print(f"Expected Frequencies Table:\n{expected}")

            if p_val < 0.05:
                print("Significant association found.")
            else:
                print("No significant association found.")

import matplotlib.pyplot as plt

# Event Trace Over Time
plt.figure(figsize=(12, 6))
event_trace['event_start_time'].hist(bins=100)
plt.title('Event Start Time Distribution')
plt.xlabel('Time')
plt.ylabel('Number of Events')
plt.show()

"""#**Feature Engineering**"""

import pandas as pd
import numpy as np

# Assume final_merged_data is the DataFrame with the provided initial features
final_merged_data['event_start_time'] = pd.to_datetime(final_merged_data['event_start_time'])
final_merged_data['event_stop_time'] = pd.to_datetime(final_merged_data['event_stop_time'])

# Feature 1: event_duration (duration of each event)
final_merged_data['event_duration'] = (final_merged_data['event_stop_time'] - final_merged_data['event_start_time']).dt.total_seconds()

# Feature 2: time_since_last_event
#final_merged_data['time_since_last_event'] = final_merged_data.groupby('node_id')['event_start_time'].diff().dt.total_seconds()

# Feature 3: time_until_next_failure
# final_merged_data['time_until_next_failure'] = (final_merged_data['event_start_time'].shift(-1) - final_merged_data['event_stop_time']).dt.total_seconds()

# Feature 4: fault_frequency (number of faults per node)
final_merged_data['fault_frequency'] = final_merged_data.groupby('node_id')['event_type'].transform(lambda x: (x == 0).sum())

# Feature 5: avg_event_duration (average duration of events per node)
final_merged_data['avg_event_duration'] = final_merged_data.groupby('node_id')['event_duration'].transform('mean')

# Feature 6: total_event_duration (total duration of all events per node)
final_merged_data['total_event_duration'] = final_merged_data.groupby('node_id')['event_duration'].transform('sum')

# Feature 7: event_count (total number of events per node)
final_merged_data['event_count'] = final_merged_data.groupby('node_id')['event_id'].transform('count')

# Feature 8: start_hour (hour of the day when the event started)
final_merged_data['start_hour'] = final_merged_data['event_start_time'].dt.hour

# Feature 9: start_day_of_week (day of the week when the event started)
#final_merged_data['start_day_of_week'] = final_merged_data['event_start_time'].dt.dayofweek

# Feature 10: availability_ratio (ratio of available events to total events per node)
#availability = final_merged_data.groupby('node_id')['event_type'].apply(lambda x: (x == 1).sum())
#total_events = final_merged_data.groupby('node_id')['event_id'].count()
#availability_ratio = availability / total_events
#final_merged_data = final_merged_data.merge(availability_ratio.rename('availability_ratio'), on='node_id', how='left')

#Feature 11: time between failures

final_merged_data.loc[final_merged_data['event_type'] == 0, 'time_between_failures'] = (
    final_merged_data.loc[final_merged_data['event_type'] == 0, 'event_stop_time'] -
    final_merged_data.loc[final_merged_data['event_type'] == 0, 'event_start_time']
).dt.total_seconds()

#Feature 12: time to repair

final_merged_data.loc[final_merged_data['event_type'] == 1, 'time_to_repair'] = (
    final_merged_data.loc[final_merged_data['event_type'] == 1, 'event_stop_time'] -
    final_merged_data.loc[final_merged_data['event_type'] == 1, 'event_start_time']
).dt.total_seconds()



# Replace NaN or missing values in engineered features with 0
final_merged_data.fillna(0, inplace=True)

# Verify the new engineered features
print(final_merged_data.head())

print(final_merged_data.head(10))

# Loop through each column and print unique values, their count, and frequency
for column in final_merged_data.columns:
    print(f"Column: {column}")
    print(f"Number of Unique Values: {final_merged_data[column].nunique()}")
    print("Value Counts:")
    print(final_merged_data[column].value_counts().head(10))  # Display top 10 most frequent values
    print("-" * 50)

# Calculate time until the next failure
#final_merged_data['time_until_next_failure'] = final_merged_data['event_start_time'] - final_merged_data['event_stop_time'].shift(-1)

# Convert timedelta to seconds
#final_merged_data['time_until_next_failure'] = final_merged_data['time_until_next_failure'].dt.total_seconds()

# Handle the first row separately
#final_merged_data['time_until_next_failure'].iloc[-1] = None  # Set last row to NaN or default value

final_merged_data.info()

"""##Normalization"""

'''
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
final_merged_data['event_duration_scaled'] = scaler.fit_transform(final_merged_data[['event_duration']]).
'''

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
final_merged_data['event_duration_scaled'] = scaler.fit_transform(final_merged_data[['event_duration']])

#### Following lines are written by Yogesh Sharma
final_merged_data['time_between_failures_scaled'] = scaler.fit_transform(final_merged_data[['time_between_failures']])
final_merged_data['time_to_repair_scaled'] = scaler.fit_transform(final_merged_data[['time_to_repair']])

final_merged_data.columns

final_merged_data.describe()

final_merged_data.info()

final_merged_data.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical columns
numerical_cols = final_merged_data.select_dtypes(include=['float64', 'int64']).columns

# Calculate the correlation matrix for numerical columns
correlation_matrix = final_merged_data[numerical_cols].corr()

# Plot the correlation matrix
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.show()

final_merged_data.columns

"""# Site wise datasets"""

import pandas as pd

# List of all site names (from the node_location values)
sites = ['site1', 'site2', 'site3', 'site4', 'site5', 'site6', 'site7', 'site8', 'site9']

# Create a dictionary to hold dataframes for each site
site_dfs = {}

# Iterate through each site and filter the dataframe by node_location
for site in sites:
    site_dfs[site] = final_merged_data[final_merged_data['node_location'].str.contains(f'/{site}/')]

# Now you can access each site's dataframe like this:
# site_dfs['site1'], site_dfs['site2'], ..., site_dfs['site9']

site1_df = site_dfs['site1']
site2_df = site_dfs['site2']
site3_df = site_dfs['site3']
site4_df = site_dfs['site4']
site5_df = site_dfs['site5']
site6_df = site_dfs['site6']
site7_df = site_dfs['site7']
site8_df = site_dfs['site8']
site9_df = site_dfs['site9']

print(site1_df.head())

# Loop through each column and print unique values, their count, and frequency
for column in site1_df.columns:
    print(f"Column: {column}")
    print(f"Number of Unique Values: {site1_df[column].nunique()}")
    print("Value Counts:")
    print(site1_df[column].value_counts().head(100))  # Display top 10 most frequent values
    print("-" * 50)

print(site2_df.head())

"""## Cluster Wise Datasets"""

# List of clusters
clusters = [
    'G1/site1/c1', 'G1/site1/c2', 'G1/site1/c3', 'G1/site1/c4',
    'G1/site2/c1', 'G1/site3/c1', 'G1/site4/c1', 'G1/site4/c2',
    'G1/site5/c1', 'G1/site5/c2', 'G1/site6/c1', 'G1/site6/c2',
    'G1/site7/c1', 'G1/site8/c1', 'G1/site9/c1'
]

cluster_column = 'node_location'

# Create a dictionary to store dataframes for each cluster
cluster_dataframes = {}

# Generate dataframes for each cluster
for cluster in clusters:
    cluster_dataframes[cluster] = final_merged_data[final_merged_data[cluster_column] == cluster]

for cluster, df in cluster_dataframes.items():
    print(f"Cluster {cluster}: {len(df)} rows")

cluster1_df = cluster_dataframes['G1/site1/c1']
cluster2_df = cluster_dataframes['G1/site1/c2']
cluster3_df = cluster_dataframes['G1/site1/c3']
cluster4_df = cluster_dataframes['G1/site1/c4']
cluster5_df = cluster_dataframes['G1/site2/c1']
cluster6_df = cluster_dataframes['G1/site3/c1']
cluster7_df = cluster_dataframes['G1/site4/c1']
cluster8_df = cluster_dataframes['G1/site4/c2']
cluster9_df = cluster_dataframes['G1/site5/c1']
cluster10_df = cluster_dataframes['G1/site5/c2']
cluster11_df = cluster_dataframes['G1/site6/c1']
cluster12_df = cluster_dataframes['G1/site6/c2']
cluster13_df = cluster_dataframes['G1/site7/c1']
cluster14_df = cluster_dataframes['G1/site8/c1']
cluster15_df = cluster_dataframes['G1/site9/c1']

print(cluster1_df.head())

print(cluster2_df.head())

"""## Node-Wise datasets for nodes with the number of highest, median and lowest event counts in each of the 15 clusters, totaling 45 nodes in all"""

# List of node_ids
node_ids = [
    9, 20, 11, 342, 447, 352, 820, 839, 780, 889, 429, 434, 1268, 887, 683,
    888, 717, 79, 454, 663, 458, 1250, 1060, 1019, 520, 523, 491, 1171, 1161,
    1141, 542, 622, 577, 1210, 1229, 1208, 840, 872, 868, 965, 273, 209, 1282,
    1101, 1078
]

node_id_column = 'node_id'

# Generate dataframes for each node_id
for node_id in node_ids:
    dataframe_name = f"node{node_id}_df"
    globals()[dataframe_name] = final_merged_data[final_merged_data[node_id_column] == node_id]

# Print summary for each dynamically created dataframe
for node_id in node_ids:
    dataframe_name = f"node{node_id}_df"
    print(f"{dataframe_name}: {len(globals()[dataframe_name])} rows")

node9_df.head()

"""# **Problem statement 1 - Prediction of Event Type (Classification)**

### Target Variable - 'event_type'
"""

# Dropping irrelevant or highly correlated columns for model input
X = final_merged_data.drop(columns=[
    'event_type','node_name', 'node_location'])  # Dropping the target from features

y = final_merged_data['event_type']

from sklearn.model_selection import train_test_split

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Pipeline"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Identify numerical and categorical columns
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object', 'bool']).columns

# Define preprocessing for numerical data (scaling)
num_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

# Define preprocessing for categorical data (one-hot encoding)
cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Combine numerical and categorical transformations
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_transformer, num_features),
        ('cat', cat_transformer, cat_features)])

# Integrate preprocessing into a full pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

"""## Model 1 - Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Append the Logistic Regression model to the pipeline
log_reg = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', LogisticRegression(max_iter=1000))])

# Train the model
log_reg.fit(X_train, y_train)

# Predict on the test set
y_pred_logreg = log_reg.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("\nClassification Report:\n", classification_report(y_test, y_pred_logreg))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_logreg))

"""### Feature importance"""

import numpy as np
import matplotlib.pyplot as plt

# Extract feature importance from Logistic Regression
logreg_importance = log_reg.named_steps['classifier'].coef_[0]  # Coefficients of the model
feature_names = log_reg.named_steps['preprocessor'].transformers_[0][2]  # Feature names from preprocessor

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, logreg_importance, color="green")
plt.xlabel("Feature Coefficient")
plt.ylabel("Feature Name")
plt.title("Feature Importance - Logistic Regression")
plt.show()

"""### Learning curve for model 1"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Define a function to plot the learning curve
def plot_learning_curve(estimator, title, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)):
    plt.figure(figsize=(10, 6))
    plt.title(title)
    plt.xlabel("Training Set Size")
    plt.ylabel("Accuracy")

    # Compute learning curve
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring, train_sizes=train_sizes, n_jobs=-1)

    # Calculate mean and standard deviation of training and test scores
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Plot learning curve
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    plt.legend(loc="best")
    plt.grid()
    plt.show()

# Plot the learning curve for the Logistic Regression model
plot_learning_curve(log_reg,
                    title="Learning Curve for Logistic Regression",
                    X=X, y=y, cv=5, scoring='accuracy')

"""## Model 1 (Logistic Regressoin) on 9 sites"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define a function to preprocess and train the model for each site
def train_and_evaluate_logistic_regression(site_name, site_df, target_column='target'):

    target_column = 'event_type'

    # Define features (X) and target (y)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location'])
    y = site_df[target_column]  # Target column

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define preprocessing for numeric and categorical columns
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ]
    )

    # Create a pipeline for Logistic Regression
    log_reg = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(max_iter=1000))
    ])

    # Train the model
    log_reg.fit(X_train, y_train)

    # Predict on the test set
    y_pred = log_reg.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    matrix = confusion_matrix(y_test, y_pred)

    # Print the results
    print(f"\nPerformance for {site_name}: ")
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:\n", report)
    print("\nConfusion Matrix:\n", matrix)


# Iterate through all 9 site DataFrames and run the model
site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

for site_name, site_df in site_dataframes.items():
    print(f"Processing data for {site_name}")
    train_and_evaluate_logistic_regression(site_name, site_df)

"""## Model 1 (Logistic Regression) on 15 clusters"""

# List of clusters
clusters = [
    'G1/site1/c1', 'G1/site1/c2', 'G1/site1/c3', 'G1/site1/c4',
    'G1/site2/c1', 'G1/site3/c1', 'G1/site4/c1', 'G1/site4/c2',
    'G1/site5/c1', 'G1/site5/c2', 'G1/site6/c1', 'G1/site6/c2',
    'G1/site7/c1', 'G1/site8/c1', 'G1/site9/c1'
]

# Initialize a dictionary to store results
cluster_results = {}

# Loop through each cluster
for cluster in clusters:
    # Filter data for the current cluster
    cluster_df = final_merged_data[final_merged_data['node_location'] == cluster]

    # Split data into features and target
    X = cluster_df.drop(columns=['event_type', 'node_name', 'node_location'])
    y = cluster_df['event_type']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    log_reg.fit(X_train, y_train)

    # Predict on the test set
    y_pred = log_reg.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)

    # Store results for the cluster
    cluster_results[cluster] = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': confusion
    }

# Print results for all clusters
for cluster, result in cluster_results.items():
    print(f"Cluster: {cluster}")
    print(f"Accuracy: {result['accuracy']}")
    print(f"Classification Report:\n{result['classification_report']}")
    print(f"Confusion Matrix:\n{result['confusion_matrix']}")
    print("-" * 50)

"""## Model 1 (Logistic Regression) on 45 nodes"""

# List of node dataframes and their corresponding IDs
node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 1268: node1268_df, 887: node887_df, 683: node683_df,
    888: node888_df, 717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1250: node1250_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1171: node1171_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1282: node1282_df, 1101: node1101_df, 1078: node1078_df
}

# Initialize a dictionary to store results
node_results = {}

for node_id, node_df in node_dataframes.items():
  print(f"Node {node_id} columns: {node_df.columns.tolist()}")

# Loop through each node dataframe
for node_id, node_df in node_dataframes.items():
    # Check if the dataframe has sufficient rows for splitting
    if len(node_df) < 10:
        print(f"Node {node_id} has insufficient data ({len(node_df)} rows). Skipping...")
        continue

    # Split data into features and target
    X = node_df.drop(columns=['event_type'])
    y = node_df['event_type']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    log_reg.fit(X_train, y_train)

    # Predict on the test set
    y_pred = log_reg.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)

    # Store results for the node
    node_results[node_id] = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': confusion
    }

# Print results for all nodes
for node_id, result in node_results.items():
    print(f"Node ID: {node_id}")
    print(f"Accuracy: {result['accuracy']}")
    print(f"Classification Report:\n{result['classification_report']}")
    print(f"Confusion Matrix:\n{result['confusion_matrix']}")
    print("-" * 50)

"""## Model 2 - Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

# Append the Random Forest model to the pipeline
rf_model = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', RandomForestClassifier(n_estimators=10, random_state=42))])

# Train the model
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

"""### Feature Importance"""

# Importances from the Rrandom Forest Model
importances_rf = rf_model.named_steps['classifier'].feature_importances_

# Numerical features
all_features = num_features.tolist()

# Ensure the length of all_features matches importances_rf
assert len(all_features) == len(importances_rf), "Mismatch between the number of features and importances."

# Sort the feature importances
sorted_indices_rf = importances_rf.argsort()

# Plot the feature importances
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_indices_rf)), importances_rf[sorted_indices_rf], align='center', color="red")
plt.yticks(range(len(sorted_indices_rf)), [all_features[i] for i in sorted_indices_rf])
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance')
plt.show()

"""### Learning curve for model 2"""

from sklearn.model_selection import learning_curve

# Generate learning curve data
train_sizes, train_scores, test_scores = learning_curve(
    estimator=rf_model, X=X_train, y=y_train, cv=3, scoring='accuracy', n_jobs=-1)

# Calculate mean and standard deviation for training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)
plt.plot(train_sizes, test_mean, label='Validation Accuracy', color='green')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='orange', alpha=0.2)
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curve - Random Forest')
plt.legend(loc='best')
plt.grid()
plt.show()

"""## Model 2 (Random Forest) on 9 sites"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define a function to preprocess and train the model for each site
def train_and_evaluate_random_forest_regression(site_name, site_df, target_column='target'):
    """
    Trains and evaluates a Logistic Regression model for the given site data.
    Parameters:
        - site_df: DataFrame containing data for a single site.
        - target_column: The column name to predict (default: 'target').
    """

    target_column = 'event_type'

    # Define features (X) and target (y)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location'])
    y = site_df[target_column]  # Target column

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train the model
    rf_model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    matrix = confusion_matrix(y_test, y_pred)

    # Print the results
    print(f"\nPerformance for {site_name}: ")
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:\n", report)
    print("\nConfusion Matrix:\n", matrix)


# Iterate through all 9 site DataFrames and run the model
site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

for site_name, site_df in site_dataframes.items():
    print(f"Processing data for {site_name}")
    train_and_evaluate_random_forest_regression(site_name, site_df)

"""## Model 2 (Random Forest) on 15 clusters"""

# List of clusters
clusters = [
    'G1/site1/c1', 'G1/site1/c2', 'G1/site1/c3', 'G1/site1/c4',
    'G1/site2/c1', 'G1/site3/c1', 'G1/site4/c1', 'G1/site4/c2',
    'G1/site5/c1', 'G1/site5/c2', 'G1/site6/c1', 'G1/site6/c2',
    'G1/site7/c1', 'G1/site8/c1', 'G1/site9/c1'
]

# Initialize a dictionary to store results
cluster_results = {}

# Loop through each cluster
for cluster in clusters:
    # Filter data for the current cluster
    cluster_df = final_merged_data[final_merged_data['node_location'] == cluster]

    # Split data into features and target
    X = cluster_df.drop(columns=['event_type'])
    y = cluster_df['event_type']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    rf_model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)

    # Store results for the cluster
    cluster_results[cluster] = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': confusion
    }

# Print results for all clusters
for cluster, result in cluster_results.items():
    print(f"Cluster: {cluster}")
    print(f"Accuracy: {result['accuracy']}")
    print(f"Classification Report:\n{result['classification_report']}")
    print(f"Confusion Matrix:\n{result['confusion_matrix']}")
    print("-" * 50)

"""## Model 2 (Random Forest) on 45 Nodes, 3 from each of 15 clusters"""

# List of node dataframes and their corresponding IDs
node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 1268: node1268_df, 887: node887_df, 683: node683_df,
    888: node888_df, 717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1250: node1250_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1171: node1171_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1282: node1282_df, 1101: node1101_df, 1078: node1078_df
}

# Initialize a dictionary to store results
node_results = {}

# Loop through each node dataframe
for node_id, node_df in node_dataframes.items():
    # Check if the dataframe has sufficient rows for splitting
    if len(node_df) < 10:
        print(f"Node {node_id} has insufficient data ({len(node_df)} rows). Skipping...")
        continue

    # Split data into features and target
    X = node_df.drop(columns=['event_type'])
    y = node_df['event_type']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model
    rf_model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = rf_model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)

    # Store results for the node
    node_results[node_id] = {
        'accuracy': accuracy,
        'classification_report': report,
        'confusion_matrix': confusion
    }

# Print results for all nodes
for node_id, result in node_results.items():
    print(f"Node ID: {node_id}")
    print(f"Accuracy: {result['accuracy']}")
    print(f"Classification Report:\n{result['classification_report']}")
    print(f"Confusion Matrix:\n{result['confusion_matrix']}")
    print("-" * 50)

"""## Model 3 - XGBoost Classifier"""

from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Extract datetime columns
datetime_columns = ['event_start_time', 'event_stop_time']

# Convert datetime columns to numerical features (e.g., timestamps)
for col in datetime_columns:
    X_train[col] = X_train[col].astype('int64') // 10**9  # Convert to Unix timestamp
    X_test[col] = X_test[col].astype('int64') // 10**9

# Drop categorical columns (object type)
categorical_columns = X_train.select_dtypes(include=['object']).columns
X_train_numeric = X_train.drop(columns=categorical_columns)
X_test_numeric = X_test.drop(columns=categorical_columns)

# Scale numeric columns only
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_numeric)
X_test_scaled = scaler.transform(X_test_numeric)

# Train the XGBoost model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_xgb = xgb_model.predict(X_test_scaled)

# Evaluate the model
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

"""### Feature Importance"""

import matplotlib.pyplot as plt

# Extract feature importance from XGBoost
xgb_importance = xgb_model.feature_importances_

# Use the original DataFrame column names (from numeric features)
transformed_feature_names = X_train_numeric.columns  # Columns from the original numeric DataFrame

# Check the number of features
print(f"Number of features in model: {len(xgb_importance)}")
print(f"Number of features in transformed data: {len(transformed_feature_names)}")

# Ensure lengths match before plotting
if len(xgb_importance) == len(transformed_feature_names):
    # Plot feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(transformed_feature_names, xgb_importance, color="blue")
    plt.xlabel("Feature Importance Score")
    plt.ylabel("Feature Name")
    plt.title("Feature Importance - XGBoost")
    plt.show()
else:
    print("Mismatch between model feature importance and transformed data columns.")

"""### Learning curve for model 3"""

import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

def plot_learning_curve_manual_xgb(X_train, X_test, y_train, y_test, datetime_columns, title="Manual Learning Curve for XGBoost"):
    # Convert datetime columns to numerical format
    for col in datetime_columns:
        X_train[col] = X_train[col].astype('int64') // 10**9  # Convert to Unix timestamp
        X_test[col] = X_test[col].astype('int64') // 10**9

    # Select only numeric columns (drop non-numeric ones)
    X_train_numeric = X_train.select_dtypes(include=['int64', 'float64'])
    X_test_numeric = X_test.select_dtypes(include=['int64', 'float64'])

    # Scale numeric columns
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_numeric)
    X_test_scaled = scaler.transform(X_test_numeric)

    # Convert labels to NumPy arrays
    y_train = y_train.to_numpy()
    y_test = y_test.to_numpy()

    # Define training set sizes
    train_sizes = np.linspace(0.1, 1.0, 10)  # 10 different sizes
    train_scores = []
    test_scores = []

    # Loop over different training sizes
    for size in train_sizes:
        # Calculate the number of samples for the current size
        n_samples = int(size * len(X_train_scaled))

        # Create subsets for the current size
        X_train_subset = X_train_scaled[:n_samples]
        y_train_subset = y_train[:n_samples]

        # Train the XGBoost model
        xgb_model = XGBClassifier(random_state=42)
        xgb_model.fit(X_train_subset, y_train_subset)

        # Evaluate on training subset
        y_train_pred = xgb_model.predict(X_train_subset)
        train_accuracy = accuracy_score(y_train_subset, y_train_pred)

        # Evaluate on the full test set
        y_test_pred = xgb_model.predict(X_test_scaled)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Store the scores
        train_scores.append(train_accuracy)
        test_scores.append(test_accuracy)

    # Plot the learning curve
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes * len(X_train_scaled), train_scores, 'o-', color="r", label="Training score")
    plt.plot(train_sizes * len(X_train_scaled), test_scores, 'o-', color="g", label="Validation score")
    plt.title(title)
    plt.xlabel("Number of Training Samples")
    plt.ylabel("Accuracy")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

plot_learning_curve_manual_xgb(X_train, X_test, y_train, y_test, datetime_columns)

"""## Model 3 on 9 sites"""

from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

def train_and_evaluate_model3(resource_name, data, target_column='event_type'):
    """
    Train and evaluate the XGBoost model for a given node.
    Handles cases where there is only one class in the target variable.
    """
    # Split features and target
    X = data.drop(columns=[target_column, 'node_name', 'node_location'])
    y = data[target_column]

    # Check for unique classes in y
    unique_classes = y.unique()
    if len(unique_classes) < 2:
        print(f"Node {resource_name} skipped: Only one class ({unique_classes[0]}) in the target.")
        return  # Exit early for this node

    # Convert datetime columns to numerical features (e.g., Unix timestamps)
    datetime_columns = ['event_start_time', 'event_stop_time']
    for col in datetime_columns:
        if col in X.columns:
            X[col] = pd.to_datetime(X[col]).astype('int64') // 10**9  # Convert to Unix timestamp

    # Split into train and test sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Select numeric columns for scaling
    numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns

    # Scale numeric features
    scaler = StandardScaler()
    X_train_scaled = X_train.copy()
    X_test_scaled = X_test.copy()
    X_train_scaled[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
    X_test_scaled[numeric_columns] = scaler.transform(X_test[numeric_columns])

    # Train the XGBoost model
    xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')  # Updated: removed 'use_label_encoder'
    xgb_model.fit(X_train_scaled, y_train)

    # Predict on the test set
    y_pred = xgb_model.predict(X_test_scaled)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    matrix = confusion_matrix(y_test, y_pred)

    # Print results
    print(f"Performance for: {resource_name}")
    print(f"Accuracy: {accuracy:.2f}")
    print("\nClassification Report:\n", report)
    print("\nConfusion Matrix:\n", matrix)

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model3(site_name, site_df)

"""## Model 3 on 15 Clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model3(cluster_name, cluster_df)

"""##Model 3 on 45 Nodes"""

node1268_df.info()

node888_df.info()

node1171_df.info()

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1250: node1250_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1282: node1282_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model3(node_id, node_df)

"""##Rainforest model Cross-validation for robustness"""

from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline

# Perform cross-validation with the Random Forest model
rf_model = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', RandomForestClassifier(n_estimators=10, random_state=42))])

# Perform 5-fold cross-validation on the training data
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=10, scoring='accuracy')

# Output the cross-validation scores and the mean accuracy
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", cv_scores.mean())

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Generate cross-validated predictions
y_pred_cv = cross_val_predict(rf_model, X_train, y_train, cv=5)

# Print the classification report
print("Classification Report for Cross-Validated Model:")
print(classification_report(y_train, y_pred_cv))

"""# **Problem statement 2 - Prediction of time-to-next-Event**"""

pip install scikit-survival

final_merged_data.head(10)

final_merged_data.info()

# Prepare the dataframe with relevant features and target
# Convert datetime columns into total seconds since epoch
final_merged_data['event_start_time'] = pd.to_datetime(final_merged_data['event_start_time'])
final_merged_data['event_stop_time'] = pd.to_datetime(final_merged_data['event_stop_time'])

final_merged_data['event_start_seconds'] = final_merged_data['event_start_time'].astype('int64') // 1e9
final_merged_data['event_stop_seconds'] = final_merged_data['event_stop_time'].astype('int64') // 1e9

# Define features (X) and target (y)
X = final_merged_data.drop(columns=[
    'node_name', 'node_location',  # Non-numeric/categorical columns
    'event_start_time', 'event_stop_time',  # Original datetime columns
    'event_start_seconds',
    'event_stop_seconds',  # Remove target from features
    'event_duration'
])
y = final_merged_data['event_stop_seconds']  # Numeric target variable

# Normalize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X.select_dtypes(include=['float64', 'int64']))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

'''
# Prepare the dataframe with relevant features and target
# Convert datetime columns into total seconds since epoch
final_merged_data['event_start_time'] = pd.to_datetime(final_merged_data['event_start_time'])
final_merged_data['event_stop_time'] = pd.to_datetime(final_merged_data['event_stop_time'])

final_merged_data['event_start_seconds'] = final_merged_data['event_start_time'].astype('int64') // 1e9
final_merged_data['event_stop_seconds'] = final_merged_data['event_stop_time'].astype('int64') // 1e9

# Define features (X) and target (y)
X = final_merged_data.drop(columns=[
    'node_name', 'node_location',  # Non-numeric/categorical columns
    'event_start_time', 'event_stop_time',  # Original datetime columns
    'event_start_seconds',
    'event_stop_seconds',  # Remove target from features
    'event_duration'
])
y = final_merged_data['event_stop_seconds']  # Numeric target variable

# Normalize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X.select_dtypes(include=['float64', 'int64']))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
'''

"""## Model 4 - Linear Regressor"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the Linear Regression model
linear_model2 = LinearRegression()

# Train the model on the training data
linear_model2.fit(X_train, y_train)

# Predict on train and test data
y_pred_train_lr = linear_model2.predict(X_train)
y_pred_test_lr = linear_model2.predict(X_test)

# Calculate evaluation metrics for training data
mae_train_lr = mean_absolute_error(y_train, y_pred_train_lr)
mse_train_lr = mean_squared_error(y_train, y_pred_train_lr)
rmse_train_lr = mse_train_lr ** 0.5
r2_train_lr = r2_score(y_train, y_pred_train_lr)

# Calculate evaluation metrics for test data
mae_test_lr = mean_absolute_error(y_test, y_pred_test_lr)
mse_test_lr = mean_squared_error(y_test, y_pred_test_lr)
rmse_test_lr = mse_test_lr ** 0.5
r2_test_lr = r2_score(y_test, y_pred_test_lr)

# Print the results
print("\nLinear Regression Results:")
print(f"Training MAE: {mae_train_lr:.2f}, RMSE: {rmse_train_lr:.2f}, R2: {r2_train_lr:.2f}")
print(f"Test MAE: {mae_test_lr:.2f}, RMSE: {rmse_test_lr:.2f}, R2: {r2_test_lr:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Generate the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    LinearRegression(),  # Model
    X_train,             # Features
    y_train,             # Target
    cv=5,                # Cross-validation folds
    scoring='r2',        # Evaluation metric (R²)
    n_jobs=-1,           # Use all available cores
    train_sizes=np.linspace(0.1, 1.0, 10)  # Training sizes from 10% to 100%
)

# Calculate mean and standard deviation of train and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, label='Training R²', color='blue', marker='o')
plt.fill_between(train_sizes,
                 train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std,
                 alpha=0.1, color='blue')
plt.plot(train_sizes, test_scores_mean, label='Validation R²', color='orange', marker='o')
plt.fill_between(train_sizes,
                 test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std,
                 alpha=0.1, color='green')

# Customize the plot
plt.title('Learning Curve: Linear Regression')
plt.xlabel('Training Set Size')
plt.ylabel('R² Score')
plt.legend(loc='best')
plt.grid()
plt.show()

# Print evaluation results from the original code
print("\nLinear Regression Results:")
print(f"Training MAE: {mae_train_lr:.2f}, RMSE: {rmse_train_lr:.2f}, R2: {r2_train_lr:.2f}")
print(f"Test MAE: {mae_test_lr:.2f}, RMSE: {rmse_test_lr:.2f}, R2: {r2_test_lr:.2f}")

from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np

# Initialize and train the Linear Regression model
linear_model2 = LinearRegression()
linear_model2.fit(X_train, y_train)

# Get the coefficients from the model (feature importance)
lr_importance = linear_model2.coef_

# Use the feature names after selecting numeric columns (preprocessed names)
feature_names = X.select_dtypes(include=['float64', 'int64']).columns

# Ensure the shape matches between feature names and coefficients
assert len(feature_names) == len(lr_importance), \
    f"Mismatch between number of features ({len(feature_names)}) and importance scores ({len(lr_importance)})!"

# Plot the feature importance for Linear Regression (coefficients)
plt.figure(figsize=(10, 6))
#plt.barh(feature_names, lr_importance, color="green")
plt.barh(feature_names, lr_importance)
plt.title("Feature Importance: Linear Regression (TBF)")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.gca().invert_yaxis()
plt.savefig('TBF_Linear_Regression_Feature_Importance.png')
plt.show()

"""## Model 4 on 9 sites"""

# Function to train and evaluate Linear Regression
def train_and_evaluate_model4(df, name):
    if not isinstance(df, pd.DataFrame):
        print(f"Error: {name} is not a valid DataFrame. Received {type(df)}")
        return

    # Preprocess the data
    df['event_start_time'] = pd.to_datetime(df['event_start_time'])
    df['event_stop_time'] = pd.to_datetime(df['event_stop_time'])

    df['event_start_seconds'] = df['event_start_time'].astype('int64') // 1e9
    df['event_stop_seconds'] = df['event_stop_time'].astype('int64') // 1e9

    X = df.drop(columns=[
        'node_name', 'node_location',
        'event_start_time', 'event_stop_time',
        'event_start_seconds', 'event_stop_seconds'
    ])
    y = df['event_stop_seconds']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X.select_dtypes(include=['float64', 'int64']))

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Train Linear Regression
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    mae_train = mean_absolute_error(y_train, y_pred_train)
    rmse_train = mean_squared_error(y_train, y_pred_train) ** 0.5
    r2_train = r2_score(y_train, y_pred_train)

    mae_test = mean_absolute_error(y_test, y_pred_test)
    rmse_test = mean_squared_error(y_test, y_pred_test) ** 0.5
    r2_test = r2_score(y_test, y_pred_test)

    print(f"\n{name} - Linear Regression Results:")
    print(f"Training MAE: {mae_train:.2f}, RMSE: {rmse_train:.2f}, R2: {r2_train:.2f}")
    print(f"Test MAE: {mae_test:.2f}, RMSE: {rmse_test:.2f}, R2: {r2_test:.2f}")

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model4(site_df, f"Site: {site_name}")

"""## Model 4 on 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model4(cluster_df, f"Site: {cluster_name}")

"""## Model 4 on Cluster 1 of Site 4 (S4/C1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model4(cluster_df, f"Site: {cluster_name}")

from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.ticker as ticker


# Initialize and train the Linear Regression model
linear_model2 = LinearRegression()
linear_model2.fit(X_train, y_train)


# Get the coefficients from the model (feature importance)
lr_importance = linear_model2.coef_

# Normalize coefficients post hoc for visualization (scale to max absolute value = 1)
max_abs_coef = np.max(np.abs(lr_importance))
lr_importance_normalized = lr_importance / max_abs_coef if max_abs_coef != 0 else lr_importance


# Use the feature names after selecting numeric columns (preprocessed names)
feature_names = X.select_dtypes(include=['float64', 'int64']).columns


# Ensure the shape matches between feature names and coefficients
assert len(feature_names) == len(lr_importance), \
    f"Mismatch between number of features ({len(feature_names)}) and importance scores ({len(lr_importance)})!"


feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': lr_importance_normalized
}).sort_values(by='Importance', ascending=False)

print('Before Filter')
print(feature_importance_df)

# Filter out features

lr_importance_df_nonzero = feature_importance_df[
    (~feature_importance_df['Importance'].between(-0.000019, -0.000213)) &  # exclude values in this range
    (feature_importance_df['Importance'].abs() >= 0.001)                   # keep abs importance >= 0.001
]

print('After Filter')
print(lr_importance_df_nonzero)

# Plot the feature importance for Linear Regression (normalized coefficients)
plt.figure(figsize=(10, 6))
bars = plt.barh(lr_importance_df_nonzero['Feature'], lr_importance_df_nonzero['Importance'], height=0.7)
plt.title("Linear Regression", fontsize=30)
plt.xlabel("Normalized Importance (±1)", fontsize=30)
plt.ylabel("Features", fontsize=30)
plt.gca().invert_yaxis()

# Reduce the number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))


plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.1:
        # Inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # At the end of the bar (just outside)
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')


plt.savefig('TBF_Linear_Regression_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""# **Model 4 on 45 nodes**



"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model4(node_df, node_id)

"""## Model 5 - Random Forest Regressor"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Initialize and train a Random Forest Regressor
rf_model = RandomForestRegressor(
    n_estimators=10,  # Increase estimators for better performance
    max_depth=5,  # Limit tree depth to prevent overfitting
    random_state=42
)
rf_model.fit(X_train, y_train)

# Predict on train and test data
y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Calculate evaluation metrics for training data
mae_train = mean_absolute_error(y_train, y_pred_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = mse_train ** 0.5
r2_train = r2_score(y_train, y_pred_train)

# Calculate evaluation metrics for test data
mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = mse_test ** 0.5
r2_test = r2_score(y_test, y_pred_test)

# Print the results
print(f"Training MAE: {mae_train:.2f}, RMSE: {rmse_train:.2f}, R2: {r2_train:.2f}")
print(f"Test MAE: {mae_test:.2f}, RMSE: {rmse_test:.2f}, R2: {r2_test:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.ensemble import RandomForestRegressor
'''
# Generate the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    RandomForestRegressor(n_estimators=10, random_state=42),  # Random Forest model
    X_train,              # Features
    y_train,              # Target
    cv=5,                 # Cross-validation folds
    scoring='r2',         # Evaluation metric (R²)
    n_jobs=-1,            # Use all available cores
    train_sizes=np.linspace(0.1, 1.0, 10)  # Training sizes from 10% to 100%
)

# Calculate mean and standard deviation of train and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, label='Training R²', color='blue', marker='o')
plt.fill_between(train_sizes,
                 train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std,
                 alpha=0.1, color='blue')
plt.plot(train_sizes, test_scores_mean, label='Validation R²', color='green', marker='o')
plt.fill_between(train_sizes,
                 test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std,
                 alpha=0.1, color='green')

# Customize the plot

plt.title('Learning Curve: Random Forest Regressor')
plt.xlabel('Training Set Size')
plt.ylabel('R² Score')
plt.legend(loc='best')
plt.grid()
plt.show()
'''

# Print feature importances (your existing code)
import pandas as pd
feature_importances = rf_model.feature_importances_
feature_names = X.select_dtypes(include=['float64', 'int64']).columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

# Plot feature importances (your existing code)
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()

"""## Model 5 on 9 sites"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Function to train and evaluate Random Forest Regressor
def train_and_evaluate_model5(df, name):
    if not isinstance(df, pd.DataFrame):
        print(f"Error: {name} is not a valid DataFrame. Received {type(df)}")
        return

    # Preprocess the data
    df['event_start_time'] = pd.to_datetime(df['event_start_time'])
    df['event_stop_time'] = pd.to_datetime(df['event_stop_time'])

    df['event_start_seconds'] = df['event_start_time'].astype('int64') // 1e9
    df['event_stop_seconds'] = df['event_stop_time'].astype('int64') // 1e9

    X = df.drop(columns=[
        'node_name', 'node_location',
        'event_start_time', 'event_stop_time',
        'event_start_seconds', 'event_stop_seconds'
    ])
    y = df['event_stop_seconds']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X.select_dtypes(include=['float64', 'int64']))

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Initialize and train Random Forest Regressor
    rf_model = RandomForestRegressor(
        n_estimators=10,  # Adjust number of trees for better performance
        max_depth=5,  # Limiting depth to prevent overfitting
        random_state=42
    )
    rf_model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred_train = rf_model.predict(X_train)
    y_pred_test = rf_model.predict(X_test)

    mae_train = mean_absolute_error(y_train, y_pred_train)
    rmse_train = mean_squared_error(y_train, y_pred_train) ** 0.5
    r2_train = r2_score(y_train, y_pred_train)

    mae_test = mean_absolute_error(y_test, y_pred_test)
    rmse_test = mean_squared_error(y_test, y_pred_test) ** 0.5
    r2_test = r2_score(y_test, y_pred_test)

    print(f"\n{name} - Random Forest Regressor Results:")
    print(f"Training MAE: {mae_train:.2f}, RMSE: {rmse_train:.2f}, R2: {r2_train:.2f}")
    print(f"Test MAE: {mae_test:.2f}, RMSE: {rmse_test:.2f}, R2: {r2_test:.2f}")

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model5(site_df, f"Site: {site_name}")

"""#Model 5 on Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model5(cluster_df, f"Site: {cluster_name}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import matplotlib.ticker as ticker


feature_importances = rf_model.feature_importances_
feature_names = X.select_dtypes(include=['float64', 'int64']).columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

print('Before Filter')
print(feature_importance_df)

# Filter out features with zero importance
#xgb_importance_df_nonzero = xgb_importance_df[xgb_importance_df['Importance'] >= 0.001]
rf_importance_df_nonzero = feature_importance_df[feature_importance_df['Importance'] >= 0.001]

print('After Filter')
print(rf_importance_df_nonzero)


# Plot feature importances (your existing code)
plt.figure(figsize=(10, 6))

bars = plt.barh(rf_importance_df_nonzero['Feature'], rf_importance_df_nonzero['Importance'])

#plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel("Importance", fontsize=30)
plt.ylabel('Features', fontsize=30)
#plt.title('Random Forest Regressor (TBF Prediction)', fontsize=30)
plt.title('Random Forest', fontsize=30)
plt.gca().invert_yaxis()

# Reduce the number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))


plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.1:
        # Inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # At the end of the bar (just outside)
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')



plt.savefig('TBF_Random_Forest_Regressor_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""## Model 5 on 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model5(cluster_df, f"Site: {cluster_name}")

"""## Model 5 on 45 nodes"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model5(node_df, node_id)

"""## Model 6 - XGBoost Regressor"""

from xgboost import XGBRegressor

# Initialize the XGBoost Regressor model
xgb_model = XGBRegressor(
    n_estimators=100,    # Number of trees
    max_depth=5,         # Maximum depth of trees
    learning_rate=0.1,   # Step size for boosting
    random_state=42      # Seed for reproducibility
)

# Train the model on the training data
xgb_model.fit(X_train, y_train)

# Predict on train and test data
y_pred_train_xgb = xgb_model.predict(X_train)
y_pred_test_xgb = xgb_model.predict(X_test)

# Calculate evaluation metrics for training data
mae_train_xgb = mean_absolute_error(y_train, y_pred_train_xgb)
mse_train_xgb = mean_squared_error(y_train, y_pred_train_xgb)
rmse_train_xgb = mse_train_xgb ** 0.5
r2_train_xgb = r2_score(y_train, y_pred_train_xgb)

# Calculate evaluation metrics for test data
mae_test_xgb = mean_absolute_error(y_test, y_pred_test_xgb)
mse_test_xgb = mean_squared_error(y_test, y_pred_test_xgb)
rmse_test_xgb = mse_test_xgb ** 0.5
r2_test_xgb = r2_score(y_test, y_pred_test_xgb)

# Print the results
print("\nXGBoost Regressor Results:")
print(f"Training MAE: {mae_train_xgb:.2f}, RMSE: {rmse_train_xgb:.2f}, R2: {r2_train_xgb:.2f}")
print(f"Test MAE: {mae_test_xgb:.2f}, RMSE: {rmse_test_xgb:.2f}, R2: {r2_test_xgb:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# Initialize XGBoost Regressor
xgb_model = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)

# Define the fractions of the training data to use
train_sizes = np.linspace(0.1, 1.0, 10)

# Lists to store train and validation scores
train_scores = []
validation_scores = []

# Loop over training sizes
for frac in train_sizes:
    # Sample the training data
    sample_size = int(frac * X_train.shape[0])
    X_train_sample = X_train[:sample_size]
    y_train_sample = y_train[:sample_size]

    # Train the model
    xgb_model.fit(X_train_sample, y_train_sample)

    # Predict on train and validation data
    y_train_pred = xgb_model.predict(X_train_sample)
    y_val_pred = xgb_model.predict(X_test)

    # Compute R² scores
    train_scores.append(r2_score(y_train_sample, y_train_pred))
    validation_scores.append(r2_score(y_test, y_val_pred))

# Convert scores to numpy arrays
train_scores = np.array(train_scores)
validation_scores = np.array(validation_scores)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores, label='Training R²', color='blue', marker='o')
plt.plot(train_sizes, validation_scores, label='Validation R²', color='green', marker='o')

# Customize the plot
plt.title('Learning Curve: XGBoost Regressor')
plt.xlabel('Training Set Fraction')
plt.ylabel('R² Score')
plt.legend(loc='best')
plt.grid()
plt.show()

import matplotlib.pyplot as plt
from xgboost import XGBRegressor, plot_importance
import pandas as pd

# Initialize the XGBoost Regressor model
xgb_model = XGBRegressor(
    n_estimators=100,    # Number of trees
    max_depth=5,         # Maximum depth of trees
    learning_rate=0.1,   # Step size for boosting
    random_state=42      # Seed for reproducibility
)

# Train the model on the training data
xgb_model.fit(X_train, y_train)

# Get feature importance from the XGBoost model
xgb_importance = xgb_model.feature_importances_

# Extract feature names after scaling
feature_names = X.select_dtypes(include=['float64', 'int64']).columns

# Ensure the number of features matches
assert len(feature_names) == len(xgb_importance), \
    f"Mismatch between number of features ({len(feature_names)}) and importance scores ({len(xgb_importance)})!"

# Create a DataFrame to map feature names to their importance
xgb_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': xgb_importance
}).sort_values(by='Importance', ascending=False)

# Print the feature importance DataFrame
print("\nXGBoost Feature Importances:")
print(xgb_importance_df)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('XGBoost Feature Importances')
plt.gca().invert_yaxis()
plt.show()

"""## Model 6 on 9 sites"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Function to train and evaluate XGBoost Regressor
def train_and_evaluate_model6(df, name):
    if not isinstance(df, pd.DataFrame):
        print(f"Error: {name} is not a valid DataFrame. Received {type(df)}")
        return

    # Preprocess the data
    df['event_start_time'] = pd.to_datetime(df['event_start_time'])
    df['event_stop_time'] = pd.to_datetime(df['event_stop_time'])

    df['event_start_seconds'] = df['event_start_time'].astype('int64') // 1e9
    df['event_stop_seconds'] = df['event_stop_time'].astype('int64') // 1e9

    X = df.drop(columns=[
       'node_name', 'node_location',
       'event_start_time', 'event_stop_time',
       'event_start_seconds', 'event_stop_seconds'
    ])
    y = df['event_stop_seconds']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X.select_dtypes(include=['float64', 'int64']))

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Initialize and train XGBoost Regressor
    xgb_model = XGBRegressor(
        n_estimators=100,    # Number of trees
        max_depth=5,          # Maximum depth of trees
        learning_rate=0.1,    # Step size for boosting
        random_state=42       # Seed for reproducibility
    )
    xgb_model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred_train_xgb = xgb_model.predict(X_train)
    y_pred_test_xgb = xgb_model.predict(X_test)

    mae_train_xgb = mean_absolute_error(y_train, y_pred_train_xgb)
    rmse_train_xgb = mean_squared_error(y_train, y_pred_train_xgb) ** 0.5
    r2_train_xgb = r2_score(y_train, y_pred_train_xgb)

    mae_test_xgb = mean_absolute_error(y_test, y_pred_test_xgb)
    rmse_test_xgb = mean_squared_error(y_test, y_pred_test_xgb) ** 0.5
    r2_test_xgb = r2_score(y_test, y_pred_test_xgb)

    print(f"\n{name} - XGBoost Regressor Results:")
    print(f"Training MAE: {mae_train_xgb:.2f}, RMSE: {rmse_train_xgb:.2f}, R2: {r2_train_xgb:.2f}")
    print(f"Test MAE: {mae_test_xgb:.2f}, RMSE: {rmse_test_xgb:.2f}, R2: {r2_test_xgb:.2f}")

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model6(site_df, f"Site: {site_name}")

"""# Model 6 on Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model6(cluster_df, f"Site: {cluster_name}")

import matplotlib.pyplot as plt
from xgboost import XGBRegressor, plot_importance
import pandas as pd

# Initialize the XGBoost Regressor model
xgb_model = XGBRegressor(
    n_estimators=100,    # Number of trees
    max_depth=5,         # Maximum depth of trees
    learning_rate=0.1,   # Step size for boosting
    random_state=42      # Seed for reproducibility
)

# Train the model on the training data
xgb_model.fit(X_train, y_train)

# Get feature importance from the XGBoost model
xgb_importance = xgb_model.feature_importances_

# Extract feature names after scaling
feature_names = X.select_dtypes(include=['float64', 'int64']).columns

# Ensure the number of features matches
assert len(feature_names) == len(xgb_importance), \
    f"Mismatch between number of features ({len(feature_names)}) and importance scores ({len(xgb_importance)})!"

# Create a DataFrame to map feature names to their importance
xgb_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': xgb_importance
}).sort_values(by='Importance', ascending=False)

# Print the feature importance DataFrame
print("Before Filter")
print(xgb_importance_df)

# Filter out features with zero importance
xgb_importance_df_nonzero = xgb_importance_df[xgb_importance_df['Importance'] >= 0.001]

print("After Filter")
print(xgb_importance_df_nonzero)

# Plot feature importance
plt.figure(figsize=(10, 6))

bars = plt.barh(xgb_importance_df_nonzero['Feature'], xgb_importance_df_nonzero['Importance'])

#plt.barh(xgb_importance_df_nonzero['Feature'], xgb_importance_df_nonzero['Importance'])
plt.xlabel('Importance', fontsize=30)
plt.ylabel('Features', fontsize=30)
#plt.title('Feature Importance: XGBoost Regressor (TBF Prediction)', fontsize=30)
#plt.title('XGBoost Regressor (TBF Prediction)', fontsize=30)
plt.title('XGBoost', fontsize=30)

# Reduce the number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)
plt.gca().invert_yaxis()


for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.01:
        # Inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # At the end of the bar (just outside)
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

plt.savefig('TBF_XGBoost_Regressor_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""## Model 6 on 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model6(cluster_df, f"Site: {cluster_name}")

"""## Model 6 on 45 nodes"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model6(node_df, node_id)

from sklearn.model_selection import cross_val_score

# Perform 10-fold cross-validation
cv_scores = cross_val_score(rf_model, X_scaled, y, cv=10, scoring='r2')

# Print cross-validation results
print(f"Cross-Validation R2 Scores: {cv_scores}")
print(f"Mean R2 Score: {np.mean(cv_scores):.2f}")
print(f"Standard Deviation of R2 Score: {np.std(cv_scores):.2f}")

"""# **Problem statement 3 - Prediction of node of failure**

## Model 7 - Logistic regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Split the data into features (X) and target (y)
X = final_merged_data.drop(columns=['node_id', 'node_name', 'node_location'])
y = final_merged_data['node_id']

# Convert datetime columns to numeric if present
if 'event_start_time' in X.columns:
    X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9
if 'event_stop_time' in X.columns:
    X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

# Ensure all columns are numeric
X = X.apply(pd.to_numeric, errors='coerce')

# Remove classes with fewer than two samples
class_counts = y.value_counts()
classes_to_remove = class_counts[class_counts < 2].index
X_filtered = X[~y.isin(classes_to_remove)]
y_filtered = y[~y.isin(classes_to_remove)]

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered
)

# Train the logistic regression model
log_reg = LogisticRegression(max_iter=100, random_state=42)
log_reg.fit(X_train, y_train)

# Evaluate the model
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""### Feature importance"""

# Plot feature importance for logistic regression
def plot_feature_importance_log_reg(model, feature_names):
    # Get the coefficients
    coef = model.coef_
    coef_mean = np.mean(coef, axis=0)  # Average coefficients across classes (if multi-class)

    # Create a DataFrame for visualization
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': coef_mean
    }).sort_values(by='Importance', ascending=False)

    # Plot the feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')
    plt.xlabel('Coefficient Value (Importance)')
    plt.ylabel('Features')
    plt.title('Feature Importance - Logistic Regression')
    plt.gca().invert_yaxis()  # Reverse the order for better readability
    plt.show()

# Plot feature importance
plot_feature_importance_log_reg(log_reg, X_train.columns)

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Define a function to preprocess and train the logistic regression model for node_id prediction
def train_and_evaluate_model7(site_name, site_df, target_column='node_id'):
    # Features (drop target and irrelevant columns)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location'])
    y = site_df[target_column]  # Target column

    # Convert datetime columns into numeric format (total seconds since epoch)
    if 'event_start_time' in X.columns:
        X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9

    if 'event_stop_time' in X.columns:
        X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

    # Drop any datetime columns that are irrelevant or already processed
    X = X.select_dtypes(exclude=['datetime64[ns]'])

    # Ensure all columns are numeric
    X = X.apply(pd.to_numeric, errors='coerce')

    # Remove classes with only one sample
    class_counts = y.value_counts()
    classes_to_remove = class_counts[class_counts == 1].index
    X_filtered = X[~y.isin(classes_to_remove)]
    y_filtered = y[~y.isin(classes_to_remove)]

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

    # Initialize and train a Logistic Regression model
    log_reg_model = LogisticRegression(max_iter=25, random_state=42)
    log_reg_model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = log_reg_model.predict(X_test)

    # Evaluate the model's performance
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for {site_name}: {accuracy}')
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

    # Display the confusion matrix
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, y_pred))

    return log_reg_model, accuracy  # Return the trained model and accuracy for further use

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model7(site_name, site_df)

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model7(cluster_name, cluster_df)

"""# Model 7 - Logistic Regression for Cluster 1 of Site 4 (s4/c1)

"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model7(cluster_name, cluster_df)

# Plot feature importance for logistic regression
def plot_feature_importance_log_reg(model, feature_names):
    # Get the coefficients
    coef = model.coef_
    coef_mean = np.mean(coef, axis=0)  # Average coefficients across classes (if multi-class)

    # Normalize coefficients so max abs value maps to ±1
    max_abs_coef = np.max(np.abs(coef_mean))
    coef_normalized = coef_mean / max_abs_coef if max_abs_coef != 0 else coef_mean


    # Create a DataFrame for visualization
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': coef_normalized
    }).sort_values(by='Importance', ascending=False)

    print("Feature importance before filtering:")
    print(feature_importance_df)

    # Filter out small importances between -0.000019 and -0.000213 (adjusted for your case)
    # and keep only abs importance >= 0.001
    filtered_df = feature_importance_df[
        (~feature_importance_df['Importance'].between(-0.000213, -0.000019)) &
        (feature_importance_df['Importance'].abs() >= 0.01)
    ]

    print("Feature importance after filtering:")
    print(filtered_df)

    # Plot the feature importance
    plt.figure(figsize=(10, 6))
    bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=0.7)
    #plt.barh(filtered_df['Feature'], filtered_df['Importance'])
    plt.xlabel('Importance', fontsize=30)
    plt.ylabel('Features', fontsize=30)
    plt.title('Logistic Regression', fontsize=30)
    plt.gca().invert_yaxis()  # Reverse the order for better readability

    # Reduce number of x-ticks for readability
    ax = plt.gca()
    ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))


    plt.xticks(fontsize=30)
    plt.yticks(fontsize=30)


    for bar in bars:
        width = bar.get_width()
        y_pos = bar.get_y() + bar.get_height() / 2
        offset_small = 0.005  # offset for small bars
        offset_large = 0.02   # larger offset for inside labels

        if abs(width) >= 0.1:
          # For larger bars, put label *inside* near bar end if positive,
          # and inside near bar start if negative,
          # but shifted a bit right for negative bars for readability.
          if width > 0:
            x_pos = width * 0.95       # inside near end
            ha = 'right'
            plt.text(x_pos, y_pos, f'{width:.3f}', ha=ha, va='center', fontsize=25, color='black')
          else:
            x_pos = width * 1.05 + offset_large  # inside near start + shift right
            ha = 'left'
            plt.text(x_pos, y_pos, f'{width:.3f}', ha=ha, va='center', fontsize=25, color='black')
        else:
          # For smaller bars, put label just outside bar end:
          # For positive bars: just right (offset positive)
          # For negative bars: just right (offset positive), so label is always outside right side
          x_pos = width + offset_small
          plt.text(x_pos, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

    plt.savefig('FNI_Logistic_Regression_Feature_Importance.png', bbox_inches='tight')
    plt.show()

# Plot feature importance
plot_feature_importance_log_reg(log_reg, X_train.columns)

"""## Model 8 - Random Forest

> Add blockquote


"""

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler

# Features (Assuming columns that represent system metrics)
#features = ['time_since_last_event', 'event_duration', 'event_count', 'fault_frequency_x', 'avg_event_duration']

# Target (Assuming 'node_id' is the label for which node failed)
target = 'node_id'

# Split the data into X (features) and y (target)
X = final_merged_data.drop(columns=[
    'node_id', 'node_name', 'node_location'])
y = final_merged_data[target]

# Convert datetime columns into numeric format (total seconds since epoch)
if 'event_start_time' in X.columns:
    X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9

if 'event_stop_time' in X.columns:
    X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

# Drop any datetime columns that are irrelevant or already processed
X = X.select_dtypes(exclude=['datetime64[ns]'])

# Ensure all columns are numeric
X = X.apply(pd.to_numeric, errors='coerce')

# Find the node_ids (classes) with only one sample
class_counts = y.value_counts()
classes_to_remove = class_counts[class_counts == 1].index

# Filter out the rows where y belongs to a class with only one sample
X_filtered = X[~y.isin(classes_to_remove)]
y_filtered = y[~y.isin(classes_to_remove)]

# Now split the filtered dataset
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)


# Define and train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=10, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Display the confusion matrix
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

# Perform cross-validation for better evaluation
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=2, scoring='accuracy')
print(f'Cross-validation Accuracy: {cv_scores.mean()}')

"""

```
# This is formatted as code
```

### Feature Importance"""

import matplotlib.pyplot as plt
import numpy as np

# Get feature importances from the trained RandomForest model
importances = rf_model.feature_importances_
feature_names = X_train.columns
indices = np.argsort(importances)[::-1]  # Sort feature importances in descending order

feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

print("Feature importance before filtering:")
print(feature_importance_df)

 #Filter out small importances between -0.000019 and -0.000213 (adjusted for your case)
    # and keep only abs importance >= 0.001
filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.1)
  ]

print("Feature importance after filtering:")
print(filtered_df)


# Assuming X_train is your training set containing the feature names
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices])  # X_train.columns should contain the feature names
plt.show()

"""# Feature Importance"""

import matplotlib.pyplot as plt
import numpy as np

# Get feature importances from the trained RandomForest model
importances = rf_model.feature_importances_
feature_names = X_train.columns
indices = np.argsort(importances)[::-1]  # Sort feature importances in descending order

feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

print("Feature importance before filtering:")
print(feature_importance_df)

 #Filter out small importances between -0.000019 and -0.000213 (adjusted for your case)
    # and keep only abs importance >= 0.001
filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.05)
  ]

print("Feature importance after filtering:")
print(filtered_df)

# Plot horizontal bar chart
plt.figure(figsize=(10, 6))
bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=.8)


plt.title("Random Forest", fontsize=30)
plt.xlabel("Importance", fontsize=30)
plt.ylabel("Features", fontsize=30)
plt.gca().invert_yaxis()  # Highest importance at top

# Reduce number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)
plt.gca().invert_yaxis()


for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.01:
        # Inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # At the end of the bar (just outside)
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

#plt.tight_layout()
plt.savefig('FNI_Random_Forest_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""## Learning curve"""

from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

def generate_learning_curve(estimator, X, y, title, train_sizes=None, cv=3, scoring="accuracy"):
    """
    Generate and plot a learning curve for a pre-trained model.

    Parameters:
        estimator: The trained model.
        X: Features (numpy array or pandas DataFrame).
        y: Target labels.
        title: Title for the learning curve.
        train_sizes: Relative or absolute numbers of training examples to use.
        cv: Number of cross-validation folds.
        scoring: Scoring metric to use.
    """
    # Set train sizes if not provided
    if train_sizes is None:
        train_sizes = np.linspace(0.1, 1.0, 5)  # Default: 5 equally spaced points

    # Generate the learning curve
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, train_sizes=train_sizes, cv=cv, scoring=scoring, n_jobs=1
    )

    # Calculate mean and std
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Plot the learning curve
    plt.figure(figsize=(10, 6))
    plt.title(title)
    plt.xlabel("Training Set Size")
    plt.ylabel("Score")
    plt.grid()

    # Plot the training and test scores
    plt.fill_between(
        train_sizes,
        train_scores_mean - train_scores_std,
        train_scores_mean + train_scores_std,
        alpha=0.1,
        color="r",
    )
    plt.fill_between(
        train_sizes,
        test_scores_mean - test_scores_std,
        test_scores_mean + test_scores_std,
        alpha=0.1,
        color="g",
    )
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    # Add legend
    plt.legend(loc="best")
    plt.show()


# Generate the learning curve for the Random Forest model
generate_learning_curve(
    rf_model, X_filtered, y_filtered,
    title="Learning Curve: Random Forest Classifier",
    cv=3, scoring="accuracy"
)

"""## Model 8 on 9 sites"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Define a function to preprocess and train the model for node_id prediction
def train_and_evaluate_node_model8(site_name, site_df, target_column='node_id'):
    # Features (drop target and irrelevant columns)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location'])
    y = site_df[target_column]  # Target column

    # Convert datetime columns into numeric format (total seconds since epoch)
    if 'event_start_time' in X.columns:
        X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9

    if 'event_stop_time' in X.columns:
        X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

    # Drop any datetime columns that are irrelevant or already processed
    X = X.select_dtypes(exclude=['datetime64[ns]'])

    # Ensure all columns are numeric
    X = X.apply(pd.to_numeric, errors='coerce')

    # Remove classes with only one sample
    class_counts = y.value_counts()
    classes_to_remove = class_counts[class_counts == 1].index
    X_filtered = X[~y.isin(classes_to_remove)]
    y_filtered = y[~y.isin(classes_to_remove)]

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

    # Initialize and train a Random Forest Classifier
    rf_model = RandomForestClassifier(n_estimators=5, random_state=42)
    rf_model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = rf_model.predict(X_test)

    # Evaluate the model's performance
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for {site_name}:')
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

    # Display the confusion matrix
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, y_pred))

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_node_model8(site_name, site_df)

"""## Model 8 on 15 Clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_node_model8(cluster_name, cluster_df)

"""# Model 8 on Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_node_model8(cluster_name, cluster_df)

"""## Model 9 - XGBoost Classifier

"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Features (drop target and irrelevant columns)
X = final_merged_data.drop(columns=['node_id', 'node_name', 'node_location'])
y = final_merged_data['node_id']  # Target column

# Convert datetime columns into numeric format (total seconds since epoch)
if 'event_start_time' in X.columns:
    X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9

if 'event_stop_time' in X.columns:
    X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

# Drop any datetime columns that are irrelevant or already processed
X = X.select_dtypes(exclude=['datetime64[ns]'])

# Ensure all columns are numeric
X = X.apply(pd.to_numeric, errors='coerce')

# Remove classes with only one sample
class_counts = y.value_counts()
classes_to_remove = class_counts[class_counts == 1].index
X_filtered = X[~y.isin(classes_to_remove)]
y_filtered = y[~y.isin(classes_to_remove)]

# Reindex the target variable to ensure consistent class labels
y_filtered = y_filtered.astype('category').cat.codes  # Remap node_id to continuous integers starting from 0

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)

# Initialize and train an XGBoost model
xgb_model = xgb.XGBClassifier(n_estimators=10, random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions on the entire dataset
y_pred_full = xgb_model.predict(X_filtered)

# Evaluate the model's performance on the test set
y_pred = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy for the full dataset (on test set): {accuracy}')
print('Classification Report (on test set):')
print(classification_report(y_test, y_pred))

# Display the confusion matrix (on test set)
print('Confusion Matrix (on test set):')
print(confusion_matrix(y_test, y_pred))

# Plot Feature Importance
plt.figure(figsize=(10, 8))
xgb.plot_importance(xgb_model, importance_type='weight', max_num_features=10, height=0.8)
plt.title('Feature Importance')
plt.show()

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt

# Function to train and evaluate XGBoost Classifier
def train_and_evaluate_model9(df, site_name):
    if not isinstance(df, pd.DataFrame):
        print(f"Error: {site_name} is not a valid DataFrame. Received {type(df)}")
        return

    # Preprocess the data
    X = df.drop(columns=['node_id', 'node_name', 'node_location'])
    y = df['node_id']  # Target column

    # Convert datetime columns into numeric format (total seconds since epoch)
    if 'event_start_time' in X.columns:
        X['event_start_time'] = pd.to_datetime(X['event_start_time']).astype('int64') // 1e9

    if 'event_stop_time' in X.columns:
        X['event_stop_time'] = pd.to_datetime(X['event_stop_time']).astype('int64') // 1e9

    # Ensure all columns are numeric
    X = X.apply(pd.to_numeric, errors='coerce')

    # Remove classes with only one sample
    class_counts = y.value_counts()
    classes_to_remove = class_counts[class_counts == 1].index
    X = X[~y.isin(classes_to_remove)]
    y = y[~y.isin(classes_to_remove)]

    # Reindex the target variable to ensure consistent class labels
    y = y.astype('category').cat.codes

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Initialize and train an XGBoost model
    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)
    xgb_model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = xgb_model.predict(X_test)

    # Evaluate the model's performance
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n{site_name} - Accuracy: {accuracy:.2f}")
    print(f"Classification Report:")
    print(classification_report(y_test, y_pred))
    print(f"Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model9(site_df, f"Site: {site_name}")

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model9(cluster_df, f"Site: {cluster_name}")

"""# Model 9 on Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model9(cluster_df, f"Site: {cluster_name}")

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

feature_names = X_train.columns
importances = xgb_model.feature_importances_

feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("Feature importances:")
print(feature_importance_df)

#Filter out small importances between -0.000019 and -0.000213 (adjusted for your case)
    # and keep only abs importance >= 0.001
filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.008)
  ]

print("Feature importance after filtering:")
print(filtered_df)

# Plot Feature Importance
plt.figure(figsize=(10, 8))
bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=0.7)
plt.xlabel('Importance', fontsize=30)
plt.ylabel('Features', fontsize=30)
plt.gca().invert_yaxis()

# Reduce x-axis ticks for readability
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))
#ax.set_ylim(-0.5, len(filtered_df) - 0.5)  # tight vertical limits
#ax.tick_params(axis='y', pad=1)  # reduce padding of y tick labels

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

# Add importance values as labels on bars
for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2
    if 0.05 <= width < 0.15:
        # Label just outside the bar, aligned left
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')
    elif width >= 0.1:
        # Label inside near the end, aligned right
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # Label outside for smaller bars, aligned left
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')


plt.title('XGBoost', fontsize=30)
#plt.tight_layout()
plt.savefig('FNI_XGBoost_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""# **Problem statement 4 - Prediction of duration of failure**"""

print(final_merged_data.columns)

final_merged_data.head(10)

!pip install --upgrade xgboost

X.info()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score

# Split the cleaned dataset into features (X) and target (y)
X = final_merged_data.drop(columns=['event_duration', 'node_name', 'node_location', 'event_stop_time', 'event_duration_scaled'])
y = final_merged_data['event_duration']

# Further split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the preprocessing pipeline
numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Define and train the Linear Regression model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('regressor', LinearRegression())])

pipeline.fit(X_train, y_train)

# Evaluate the model
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Calculate evaluation metrics
mae_train = mean_absolute_error(y_train, y_pred_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = mse_train ** 0.5
r2_train = r2_score(y_train, y_pred_train)

mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = mse_test ** 0.5
r2_test = r2_score(y_test, y_pred_test)

# Cross-validation
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2')

# Print results
print("Model: Linear Regression")
print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")
print(f"Cross-validation R2 scores: {cv_scores}")
print(f"Mean Cross-validation R2: {cv_scores.mean()}")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve

# Generate Learning Curve
train_sizes, train_scores, test_scores = learning_curve(
    pipeline, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=3, scoring='r2', n_jobs=-1
)

# Calculate mean and std deviation of scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.title("Learning Curve: Linear Regression")
plt.xlabel("Training Set Size")
plt.ylabel("R2 Score")
plt.legend(loc="best")
plt.grid()
plt.show()

# Feature Importance
# Extract the feature names after preprocessing
preprocessor.fit(X_train)
feature_names = numeric_features  # Since there are no categorical features, use only numeric features

# Extract the coefficients of the trained linear regression model
pipeline.named_steps['regressor'].fit(preprocessor.transform(X_train), y_train)
coefficients = pipeline.named_steps['regressor'].coef_

# Plot feature importance based on coefficients
plt.figure(figsize=(12, 6))
sorted_indices = np.argsort(np.abs(coefficients))[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_coefficients = coefficients[sorted_indices]

plt.barh(sorted_feature_names[:10], sorted_coefficients[:10], color="b")
plt.title("Feature Importance (Top 10 Features)")
plt.xlabel("Coefficient Value")
plt.gca().invert_yaxis()
plt.grid()
plt.show()

"""## Model 10 through the 9 sites"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

target_column = 'event_duration'

# Define a function to preprocess and train the linear regression model for event_duration prediction
def train_and_evaluate_model10(site_name, site_df, target_column='event_duration'):
    # Features (drop target and irrelevant columns)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location', 'event_stop_time', 'event_duration_scaled'])
    y = site_df[target_column]  # Target column

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define the preprocessing pipeline
    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer(
        transformers=[('num', numeric_transformer, numeric_features),
                      ('cat', categorical_transformer, categorical_features)])

    # Define and train the Linear Regression model
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', LinearRegression())])

    pipeline.fit(X_train, y_train)

    # Evaluate the model
    y_pred_train = pipeline.predict(X_train)
    y_pred_test = pipeline.predict(X_test)

    # Calculate evaluation metrics
    mae_train = mean_absolute_error(y_train, y_pred_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = mse_train ** 0.5
    r2_train = r2_score(y_train, y_pred_train)

    mae_test = mean_absolute_error(y_test, y_pred_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = mse_test ** 0.5
    r2_test = r2_score(y_test, y_pred_test)

    # Cross-validation
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2')

    # Print results
    print(f"Model: Linear Regression - {site_name}")
    print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
    print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")
    print(f"Cross-validation R2 scores: {cv_scores}")
    print(f"Mean Cross-validation R2: {cv_scores.mean()}")

# Define your site dataframes
site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model10(site_name, site_df)

"""## Model 10 through 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model10(cluster_name, cluster_df)

"""# Model 10 for Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model10(cluster_name, cluster_df)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve

# Feature Importance
# Extract the feature names after preprocessing
preprocessor.fit(X_train)
feature_names = numeric_features  # Since there are no categorical features, use only numeric features

# Extract the coefficients of the trained linear regression model
pipeline.named_steps['regressor'].fit(preprocessor.transform(X_train), y_train)
coefficients = pipeline.named_steps['regressor'].coef_

# Normalize coefficients to range between -1 and +1 by dividing by max absolute value
max_abs_coef = np.max(np.abs(coefficients))
coefficients_normalized = coefficients / max_abs_coef if max_abs_coef != 0 else coefficients


# Sort by absolute value descending using normalized coefficients
sorted_indices = np.argsort(np.abs(coefficients_normalized))[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_coefficients = coefficients_normalized[sorted_indices]

# Create DataFrame and sort by absolute normalized importance descending
feature_importance_df = pd.DataFrame({
    'Feature': sorted_feature_names,
    'Importance': sorted_coefficients
}).sort_values(by='Importance', key=lambda x: np.abs(x), ascending=False)

print("Feature importances (normalized):")
print(feature_importance_df)

# Define your cutoff feature name exactly as in the DataFrame
cutoff_feature = 'event_stop_seconds'  # replace with your exact column name if different

# Find the position (index) of the cutoff feature in the sorted DataFrame index
if cutoff_feature in feature_importance_df['Feature'].values:
    cutoff_pos = feature_importance_df.index[feature_importance_df['Feature'] == cutoff_feature][0]

    # Select all features from start up to and including the cutoff feature
    cutoff_idx = feature_importance_df.index.get_loc(cutoff_pos)

    # Filter rows up to cutoff_idx inclusive
    feature_importance_df_filtered = feature_importance_df.iloc[:cutoff_idx + 1]
else:
    # If cutoff feature not found, output warning and keep all features
    print(f"Cutoff feature '{cutoff_feature}' not found. No filtering applied.")
    feature_importance_df_filtered = feature_importance_df

print("\nFeature importances after applying cutoff:")
print(feature_importance_df_filtered)

#plt.xticks(fontsize=30)
#plt.yticks(fontsize=30)


# Plot feature importance based on coefficients
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_df_filtered['Feature'], feature_importance_df_filtered['Importance'], height=0.7)
plt.title("Linear Regression ", fontsize=30)
plt.xlabel("Normalized Importance (±1)", fontsize=30)
plt.gca().invert_yaxis()
#plt.grid()

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.1:
        # Inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # At the end of the bar (just outside)
        plt.text(width + 0.001, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

plt.savefig('TTR_Linear_Regression_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""## Model 10 through 45 nodes"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model10(node_id, node_df)

from sklearn.ensemble import RandomForestRegressor

# Define and train the Random Forest Regressor model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('regressor', RandomForestRegressor(n_estimators=5, random_state=42))])

pipeline.fit(X_train, y_train)

# Evaluate the model
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Calculate evaluation metrics
mae_train = mean_absolute_error(y_train, y_pred_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = mse_train ** 0.5
r2_train = r2_score(y_train, y_pred_train)

mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = mse_test ** 0.5
r2_test = r2_score(y_test, y_pred_test)

# Cross-validation
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2')

# Print results
print("Model: Random Forest Regressor")
print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")
print(f"Cross-validation R2 scores: {cv_scores}")
print(f"Mean Cross-validation R2: {cv_scores.mean()}")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import learning_curve

# Generate Learning Curve
train_sizes, train_scores, test_scores = learning_curve(
    pipeline, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 5), cv=3, scoring='r2', n_jobs=-1
)

# Calculate mean and std deviation of scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.title("Learning Curve: Random Forest Regressor")
plt.xlabel("Training Set Size")
plt.ylabel("R2 Score")
plt.legend(loc="best")
plt.grid()
plt.show()

# Feature Importance
# Extract the feature importances from the trained Random Forest model
regressor = pipeline.named_steps['regressor']
feature_importances = regressor.feature_importances_

# Extract the feature names
feature_names = numeric_features  # Since there are no categorical features

# Plot feature importance
plt.figure(figsize=(12, 6))
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_importances = feature_importances[sorted_indices]

plt.barh(sorted_feature_names[:10], sorted_importances[:10], color="b")
plt.title("Feature Importance (Top 10 Features) - Random Forest")
plt.xlabel("Importance Score")
plt.gca().invert_yaxis()
plt.grid()
plt.show()

"""## Model 11 through 9 sites"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define a function to preprocess and train the Random Forest Regressor model for event_duration prediction
def train_and_evaluate_model11(site_name, site_df, target_column='event_duration'):
    # Features (drop target and irrelevant columns)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location', 'event_stop_time', 'event_duration_scaled'])
    y = site_df[target_column]  # Target column

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define the preprocessing pipeline
    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer(
        transformers=[('num', numeric_transformer, numeric_features),
                      ('cat', categorical_transformer, categorical_features)])

    # Define and train the Random Forest Regressor model
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', RandomForestRegressor(n_estimators=5, random_state=42))])

    pipeline.fit(X_train, y_train)

    # Evaluate the model
    y_pred_train = pipeline.predict(X_train)
    y_pred_test = pipeline.predict(X_test)

    # Calculate evaluation metrics
    mae_train = mean_absolute_error(y_train, y_pred_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = mse_train ** 0.5
    r2_train = r2_score(y_train, y_pred_train)

    mae_test = mean_absolute_error(y_test, y_pred_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = mse_test ** 0.5
    r2_test = r2_score(y_test, y_pred_test)

    # Cross-validation
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2')

    # Print results
    print(f"Model: Random Forest Regressor - {site_name}")
    print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
    print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")
    print(f"Cross-validation R2 scores: {cv_scores}")
    print(f"Mean Cross-validation R2: {cv_scores.mean()}")

# Define your site dataframes
site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model11(site_name, site_df)

"""## Model 11 through 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model11(cluster_name, cluster_df)

"""# Model 11 for Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
    'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model11(cluster_name, cluster_df)

# Sort features by importance descending
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_importances = feature_importances[sorted_indices]

# Create and print a DataFrame of features and their importances (top 10 or all)
import pandas as pd

feature_importance_df = pd.DataFrame({
    'Feature': sorted_feature_names,
    'Importance': sorted_importances
})

print("Feature importance before filtering:")
print(feature_importance_df)

filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.001)
  ]

print("Feature importance after filtering:")
print(filtered_df)

# Plot horizontal bar chart
plt.figure(figsize=(10, 6))
bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=.8)


plt.title("Random Forest", fontsize=30)
plt.xlabel("Importance", fontsize=30)
plt.ylabel("Features", fontsize=30)

# Reduce number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

plt.gca().invert_yaxis()  # Highest importance at top

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.067889:
        # Label inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # Label just outside and to the right of the bar
        plt.text(width + 0.01, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

#plt.tight_layout()
plt.savefig('TTR_Random_Forest_Feature_Importance.png', bbox_inches='tight')
plt.show()
# Plot feature importance
#plt.figure(figsize=(10, 6))
#sorted_indices = np.argsort(feature_importances)[::-1]
#sorted_feature_names = feature_names[sorted_indices]
#sorted_importances = feature_importances[sorted_indices]

#plt.barh(sorted_feature_names[:10], sorted_importances[:10])
#plt.title("Feature Importance (Top 10 Features) - Random Forest")
#plt.xlabel("Importance Score")
#plt.gca().invert_yaxis()
#plt.grid()
#plt.show()

"""## Model 11 through 45 nodes"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model11(node_id, node_df)

"""## Model 12 - XGBoost"""

from xgboost import XGBRegressor

# Preprocess the training and test data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Define and train the XGBoost Regressor model
xgb_regressor = XGBRegressor(n_estimators=5, random_state=42)
xgb_regressor.fit(X_train_processed, y_train)

# Evaluate the model
y_pred_train = xgb_regressor.predict(X_train_processed)
y_pred_test = xgb_regressor.predict(X_test_processed)

# Calculate evaluation metrics
mae_train = mean_absolute_error(y_train, y_pred_train)
mse_train = mean_squared_error(y_train, y_pred_train)
rmse_train = mse_train ** 0.5
r2_train = r2_score(y_train, y_pred_train)

mae_test = mean_absolute_error(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = mse_test ** 0.5
r2_test = r2_score(y_test, y_pred_test)

# Print results
print("Model: XGBoost Regressor")
print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import KFold
from xgboost import XGBRegressor
from sklearn.metrics import r2_score

# Preprocess the training data
X_train_processed = preprocessor.fit_transform(X_train)
y_train_processed = y_train.values  # Ensure y_train is a numpy array

# Define and train the XGBoost Regressor model
xgb_regressor = XGBRegressor(n_estimators=5, random_state=42)

# Set up cross-validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Store the training and test scores
train_scores = []
test_scores = []

# Loop over different training set sizes
for train_size in np.linspace(0.1, 1.0, 5):
    # Sample the training data according to the current train size
    num_train_samples = int(train_size * len(X_train_processed))
    X_train_sampled = X_train_processed[:num_train_samples]
    y_train_sampled = y_train_processed[:num_train_samples]

    # Initialize lists to store scores for this training size
    fold_train_scores = []
    fold_test_scores = []

    # Cross-validation within the sampled data
    for train_idx, val_idx in kf.split(X_train_sampled):
        X_train_fold, X_val_fold = X_train_sampled[train_idx], X_train_sampled[val_idx]
        y_train_fold, y_val_fold = y_train_sampled[train_idx], y_train_sampled[val_idx]

        # Train the model
        xgb_regressor.fit(X_train_fold, y_train_fold)

        # Evaluate on the training fold
        y_train_pred = xgb_regressor.predict(X_train_fold)
        fold_train_scores.append(r2_score(y_train_fold, y_train_pred))

        # Evaluate on the validation fold
        y_val_pred = xgb_regressor.predict(X_val_fold)
        fold_test_scores.append(r2_score(y_val_fold, y_val_pred))

    # Calculate mean scores for this training size
    train_scores.append(np.mean(fold_train_scores))
    test_scores.append(np.mean(fold_test_scores))

# Plot the learning curve
train_sizes = np.linspace(0.1, 1.0, 5)
train_scores_mean = np.array(train_scores)
test_scores_mean = np.array(test_scores)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.title("Learning Curve: XGBoost Regressor")
plt.xlabel("Training Set Size")
plt.ylabel("R2 Score")
plt.legend(loc="best")
plt.grid()
plt.show()

# Feature Importance
# Extract the feature importances from the trained XGBoost model
feature_importances = xgb_regressor.feature_importances_

# Extract the feature names
feature_names = numeric_features  # Since there are no categorical features

# Plot feature importance
plt.figure(figsize=(12, 6))
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_importances = feature_importances[sorted_indices]

plt.barh(sorted_feature_names[:10], sorted_importances[:10], color="b")
plt.title("Feature Importance (Top 10 Features) - XGBoost")
plt.xlabel("Importance Score")
plt.gca().invert_yaxis()
plt.grid()
plt.show()

X_train.columns

"""## Model 12 through 9 sites"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define a function to preprocess and train the XGBoost Regressor model for event_duration prediction
def train_and_evaluate_model12(site_name, site_df, target_column='event_duration'):
    # Features (drop target and irrelevant columns)
    X = site_df.drop(columns=[target_column, 'node_name', 'node_location', 'event_stop_time', 'event_duration_scaled'])
    y = site_df[target_column]  # Target column

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define the preprocessing pipeline
    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer(
        transformers=[('num', numeric_transformer, numeric_features),
                      ('cat', categorical_transformer, categorical_features)])

    # Preprocess the training and test data
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)

    # Define and train the XGBoost Regressor model
    xgb_regressor = XGBRegressor(n_estimators=5, random_state=42)
    xgb_regressor.fit(X_train_processed, y_train)

    # Evaluate the model
    y_pred_train = xgb_regressor.predict(X_train_processed)
    y_pred_test = xgb_regressor.predict(X_test_processed)

    # Calculate evaluation metrics
    mae_train = mean_absolute_error(y_train, y_pred_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    rmse_train = mse_train ** 0.5
    r2_train = r2_score(y_train, y_pred_train)

    mae_test = mean_absolute_error(y_test, y_pred_test)
    mse_test = mean_squared_error(y_test, y_pred_test)
    rmse_test = mse_test ** 0.5
    r2_test = r2_score(y_test, y_pred_test)

    # Print results
    print(f"Model: XGBoost Regressor - {site_name}")
    print(f"Training MAE: {mae_train}, RMSE: {rmse_train}, R2: {r2_train}")
    print(f"Test MAE: {mae_test}, RMSE: {rmse_test}, R2: {r2_test}")

# Define your site dataframes
site_dataframes = {
    'site1': site1_df, 'site2': site2_df, 'site3': site3_df,
    'site4': site4_df, 'site5': site5_df, 'site6': site6_df,
    'site7': site7_df, 'site8': site8_df, 'site9': site9_df
}

# Iterate over sites
for site_name, site_df in site_dataframes.items():
    print(f"\nProcessing data for {site_name}")
    train_and_evaluate_model12(site_name, site_df)

"""## Model 12 through 15 clusters"""

cluster_dataframes = {
    'G1/site1/c1': cluster1_df, 'G1/site1/c2': cluster2_df, 'G1/site1/c3': cluster3_df, 'G1/site1/c4': cluster4_df,
    'G1/site2/c1': cluster5_df, 'G1/site3/c1': cluster6_df, 'G1/site4/c1': cluster7_df, 'G1/site4/c2': cluster8_df,
    'G1/site5/c1': cluster9_df, 'G1/site5/c2': cluster10_df, 'G1/site6/c1': cluster11_df, 'G1/site6/c2': cluster12_df,
    'G1/site7/c1': cluster13_df, 'G1/site8/c1': cluster14_df, 'G1/site9/c1': cluster15_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model12(cluster_name, cluster_df)

"""# Model 12 for Cluster 1 of Site 4 (s4/c1)"""

cluster_dataframes = {
  'G1/site4/c1': cluster7_df
}

for cluster_name, cluster_df in cluster_dataframes.items():
    print(f"\nProcessing data for {cluster_name}")
    train_and_evaluate_model12(cluster_name, cluster_df)

# Sort features by importance descending
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_importances = feature_importances[sorted_indices]

# Create and print a DataFrame of features and their importances (top 10 or all)
import pandas as pd

feature_importance_df = pd.DataFrame({
    'Feature': sorted_feature_names,
    'Importance': sorted_importances
})

print("Feature importance before filtering:")
print(feature_importance_df)

filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.001)
  ]

print("Feature importance after filtering:")
print(filtered_df)

# Plot horizontal bar chart
plt.figure(figsize=(10, 6))
bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=.8)


plt.title("Random Forest", fontsize=30)
plt.xlabel("Importance", fontsize=30)
plt.ylabel("Features", fontsize=30)

# Reduce number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

plt.gca().invert_yaxis()  # Highest importance at top

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.067889:
        # Label inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # Label just outside and to the right of the bar
        plt.text(width + 0.01, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

#plt.tight_layout()
plt.savefig('TTR_Random_Forest_Feature_Importance.png', bbox_inches='tight')
plt.show()

# Feature Importance
# Extract the feature importances from the trained XGBoost model
feature_importances = xgb_regressor.feature_importances_

# Extract the feature names
feature_names = numeric_features  # Since there are no categorical features

# Plot feature importance
plt.figure(figsize=(12, 6))
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_importances = feature_importances[sorted_indices]

# Create a DataFrame for readable display
feature_importance_df = pd.DataFrame({
    'Feature': sorted_feature_names,
    'Importance': sorted_importances
})

print("Feature importance before filtering:")
print(feature_importance_df)

filtered_df = feature_importance_df[
    (feature_importance_df['Importance'].abs() >= 0.001)
  ]

print("Feature importance after filtering:")
print(filtered_df)

# Plot horizontal bar chart
plt.figure(figsize=(10, 6))
bars = plt.barh(filtered_df['Feature'], filtered_df['Importance'], height=.8)


plt.title("XGB", fontsize=30)
plt.xlabel("Importance", fontsize=30)
plt.ylabel("Features", fontsize=30)


# Reduce number of x-ticks to prevent overlap
ax = plt.gca()
ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=5))

plt.xticks(fontsize=30)
plt.yticks(fontsize=30)

plt.gca().invert_yaxis()  # Highest importance at top

for bar in bars:
    width = bar.get_width()
    y_pos = bar.get_y() + bar.get_height() / 2

    if width >= 0.067889:
        # Label inside the bar near the end (95% of width)
        plt.text(width * 0.95, y_pos, f'{width:.3f}', ha='right', va='center', fontsize=25, color='black')
    else:
        # Label just outside and to the right of the bar
        plt.text(width + 0.01, y_pos, f'{width:.3f}', ha='left', va='center', fontsize=25, color='black')

#plt.tight_layout()
plt.savefig('TTR_XGB_Feature_Importance.png', bbox_inches='tight')
plt.show()

"""## Model 12 through 45 nodes"""

node_dataframes = {
    9: node9_df, 20: node20_df, 11: node11_df, 342: node342_df, 447: node447_df,
    352: node352_df, 820: node820_df, 839: node839_df, 780: node780_df, 889: node889_df,
    429: node429_df, 434: node434_df, 887: node887_df, 683: node683_df,
    717: node717_df, 79: node79_df, 454: node454_df, 663: node663_df,
    458: node458_df, 1060: node1060_df, 1019: node1019_df, 520: node520_df,
    523: node523_df, 491: node491_df, 1161: node1161_df, 1141: node1141_df,
    542: node542_df, 622: node622_df, 577: node577_df, 1210: node1210_df, 1229: node1229_df,
    1208: node1208_df, 840: node840_df, 872: node872_df, 868: node868_df, 965: node965_df,
    273: node273_df, 209: node209_df, 1101: node1101_df, 1078: node1078_df
}

# Nodes 1268, 888, 1171 removed owing to only 2 entries each
# Node 1250, 1282 only one entry
for node_id, node_df in node_dataframes.items():
    print(f"\nProcessing data for Node {node_id}")
    train_and_evaluate_model12(node_id, node_df)